// AUTOMATICALLY GENERATED FILE, DO NOT EDIT
#include "caffe/common.hpp"
#ifdef USE_GREENTEA
#include "caffe/greentea/cl_kernels.hpp"
#include <sstream>
#include <string>
namespace caffe {
std::string header = "#ifndef __OPENCL_VERSION__\n#define __kernel\n#define __global\n#define __constant\n#define __local\n#define get_global_id(x) 0\n#define get_global_size(x) 0\n#define get_local_id(x) 0\n#define get_local_size(x) 0\n#define FLT_MAX 0\n#define FLT_MIN 0\n#define cl_khr_fp64\n#define cl_amd_fp64\n#define DOUBLE_SUPPORT_AVAILABLE\n#define CLK_LOCAL_MEM_FENCE\n#define Dtype float\n#define barrier(x)\n#define atomic_cmpxchg(x, y, z) x\n#endif\n\n// Types used for parameters, offset computations and so on\n#define int_tp long\n#define uint_tp unsigned long\n\n// Definitions used to cast the types above as needed\n#define int_tpc long\n#define uint_tpc unsigned long\n\n#define CONCAT(A,B) A##_##B\n#define TEMPLATE(name,type) CONCAT(name,type)\n\n#define TYPE_FLOAT 1\n#define TYPE_DOUBLE 2\n\n#if defined(cl_khr_fp64)\n#pragma OPENCL EXTENSION cl_khr_fp64 : enable\n#define DOUBLE_SUPPORT_AVAILABLE\n#elif defined(cl_amd_fp64)\n#pragma OPENCL EXTENSION cl_amd_fp64 : enable\n#define DOUBLE_SUPPORT_AVAILABLE\n#endif\n\n#if defined(cl_khr_int64_base_atomics)\n#pragma OPENCL EXTENSION cl_khr_int64_base_atomics : enable\n#define ATOMICS_64_AVAILABLE\n#endif";  // NOLINT
std::string header_float = "#ifndef __OPENCL_VERSION__\n#define __kernel\n#define __global\n#define __constant\n#define __local\n#define get_global_id(x) 0\n#define get_global_size(x) 0\n#define get_local_id(x) 0\n#define get_local_size(x) 0\n#define FLT_MAX 0\n#define FLT_MIN 0\n#define cl_khr_fp64\n#define cl_amd_fp64\n#define DOUBLE_SUPPORT_AVAILABLE\n#define CLK_LOCAL_MEM_FENCE\n#define Dtype float\n#define barrier(x)\n#define atomic_cmpxchg(x, y, z) x\n#endif\n\n// Types used for parameters, offset computations and so on\n#define int_tp long\n#define uint_tp unsigned long\n\n// Definitions used to cast the types above as needed\n#define int_tpc long\n#define uint_tpc unsigned long\n\n#define CONCAT(A,B) A##_##B\n#define TEMPLATE(name,type) CONCAT(name,type)\n\n#define TYPE_FLOAT 1\n#define TYPE_DOUBLE 2\n\n#if defined(cl_khr_fp64)\n#pragma OPENCL EXTENSION cl_khr_fp64 : enable\n#define DOUBLE_SUPPORT_AVAILABLE\n#elif defined(cl_amd_fp64)\n#pragma OPENCL EXTENSION cl_amd_fp64 : enable\n#define DOUBLE_SUPPORT_AVAILABLE\n#endif\n\n#if defined(cl_khr_int64_base_atomics)\n#pragma OPENCL EXTENSION cl_khr_int64_base_atomics : enable\n#define ATOMICS_64_AVAILABLE\n#endif";  // NOLINT
std::string activation_float = "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(relu_forward,Dtype)(const int_tp n,\n                                           __global const Dtype* in,\n                                           __global Dtype* out,\n                                           Dtype negative_slope) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    out[index] = in[index] > 0 ? in[index] : in[index] * negative_slope;\n  }\n}\n\n__kernel void TEMPLATE(relu_backward,Dtype)(const int_tp n,\n                                            __global const Dtype* in_diff,\n                                            __global const Dtype* in_data,\n                                            __global Dtype* out_diff,\n                                            Dtype negative_slope) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    out_diff[index] = in_diff[index]\n        * ((in_data[index] > 0) + (in_data[index] <= 0) * negative_slope);\n  }\n}\n\n__kernel void TEMPLATE(tanh_forward,Dtype)(const int_tp n,\n                                           __global const Dtype* in,\n                                           __global Dtype* out) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    out[index] = tanh(in[index]);\n  }\n}\n\n__kernel void TEMPLATE(tanh_backward,Dtype)(const int_tp n,\n                                            __global const Dtype* in_diff,\n                                            __global const Dtype* out_data,\n                                            __global Dtype* out_diff) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    Dtype tanhx = out_data[index];\n    out_diff[index] = in_diff[index] * (1 - tanhx * tanhx);\n  }\n}\n\n__kernel void TEMPLATE(sigmoid_forward,Dtype)(const int_tp n,\n                                              __global const Dtype* in,\n                                              __global Dtype* out) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    out[index] = 1. / (1. + exp(-in[index]));\n  }\n}\n\n__kernel void TEMPLATE(sigmoid_backward,Dtype)(const int_tp n,\n                                               __global const Dtype* in_diff,\n                                               __global const Dtype* out_data,\n                                               __global Dtype* out_diff) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    const Dtype sigmoid_x = out_data[index];\n    out_diff[index] = in_diff[index] * sigmoid_x * (1 - sigmoid_x);\n  }\n}\n\n__kernel void TEMPLATE(threshold,Dtype)(const int_tp n, const Dtype threshold,\n                                        __global const Dtype* in,\n                                        __global Dtype* out) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    out[index] = in[index] > threshold ? 1 : 0;\n  }\n}\n\n__kernel void TEMPLATE(prelu_forward,Dtype)(const int_tp n, const int_tp channels,\n                                            const int_tp dim,\n                                            __global const Dtype* in,\n                                            __global Dtype* out,\n                                            __global const Dtype* slope_data,\n                                            const int_tp div_factor) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    int_tp c = (index / dim) % channels / div_factor;\n    out[index] = in[index] > 0 ? in[index] : in[index] * slope_data[c];\n  }\n}\n\n__kernel void TEMPLATE(prelu_backward,Dtype)(const int_tp n, const int_tp channels,\n                                             const int_tp dim,\n                                             __global const Dtype* in_diff,\n                                             __global const Dtype* in_data,\n                                             __global Dtype* out_diff,\n                                             __global const Dtype* slope_data,\n                                             const int_tp div_factor) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    int_tp c = (index / dim) % channels / div_factor;\n    out_diff[index] = in_diff[index]\n        * ((in_data[index] > 0) + (in_data[index] <= 0) * slope_data[c]);\n  }\n}\n\n__kernel void TEMPLATE(prelu_param_backward,Dtype)(const int_tp n, const int_tp rows,\n                                                   const int_tp rowPitch,\n                                                   __global const Dtype* in_diff,\n                                                   __global const Dtype* in_data,\n                                                   __global Dtype* out_diff) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    out_diff[index] = in_diff[index] * in_data[index] * (in_data[index] <= 0);\n    for (int k = 1; k < rows; k++) {\n      out_diff[index] += in_diff[index + k * rowPitch]\n          * in_data[index + k * rowPitch]\n          * (in_data[index + k * rowPitch] <= 0);\n    }\n  }\n}";  // NOLINT
std::string auxiliary_float = "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(gpu_set,Dtype)(const int_tp n, const Dtype alpha, __global Dtype* y) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    y[index] = alpha;\n  }\n}";  // NOLINT
std::string batch_reindex_float = "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(br_forward,Dtype)(const int_tp count, const int_tp inner_dim,\n                                         __global const Dtype* in,\n                                         __global const Dtype* permut,\n                                         __global Dtype* out) {\n  for (int_tp index = get_global_id(0); index < count;\n      index += get_global_size(0)) {\n    int_tp n = index / (inner_dim);\n    int_tp in_n = (int_tp) (permut[n]);\n    out[index] = in[in_n * (inner_dim) + index % (inner_dim)];\n  }\n}\n\n__kernel void TEMPLATE(br_backward,Dtype)(const int_tp count, const int_tp inner_dim,\n                                          __global const Dtype* in,\n                                          __global const Dtype* top_indexes,\n                                          __global const Dtype* begins,\n                                          __global const Dtype* counts,\n                                          __global Dtype* out) {\n  for (int_tp index = get_global_id(0); index < count;\n      index += get_global_size(0)) {\n    int_tp n = index / (inner_dim);\n    out[index] = 0;\n    int_tp lower = (int_tp) (begins[n]);\n    int_tp upper = lower + (int_tp) (counts[n]);\n    for (int_tp i = lower; i < upper; ++i) {\n      int_tp in_n = (int_tp) (top_indexes[i]);\n      out[index] += in[in_n * (inner_dim) + index % (inner_dim)];\n    }\n  }\n}";  // NOLINT
std::string bnll_float = "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(bnll_forward,Dtype)(const int_tp n,\n                                           __global const Dtype* in,\n                                           __global Dtype* out) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    out[index] =\n        in[index] > 0 ?\n            in[index] + log(1. + exp(-in[index])) : log(1. + exp(in[index]));\n  }\n}\n\n__kernel void TEMPLATE(bnll_backward,Dtype)(const int_tp n,\n                                            __global const Dtype* in_diff,\n                                            __global const Dtype* in_data,\n                                            __global Dtype* out_diff) {\n  Dtype kBNLL_THRESHOLD = 50.;\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    Dtype expval = exp(min(in_data[index], kBNLL_THRESHOLD));\n    out_diff[index] = in_diff[index] * expval / (expval + 1.);\n  }\n}";  // NOLINT
std::string channel_float = "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(kernel_channel_max,Dtype)(const int_tp num, const int_tp channels,\n                                   const int_tp spatial_dim,\n                                   __global const Dtype* data,\n                                   __global Dtype* out) {\n  for (int_tp index = get_global_id(0); index < num * spatial_dim; index +=\n      get_global_size(0)) {\n    int_tp n = index / spatial_dim;\n    int_tp s = index % spatial_dim;\n    float maxval = -FLT_MAX;\n    for (int_tp c = 0; c < channels; ++c) {\n      maxval = max((Dtype)(data[(n * channels + c) * spatial_dim + s]), (Dtype)maxval);\n    }\n    out[index] = maxval;\n  }\n}\n\n__kernel void TEMPLATE(kernel_channel_subtract,Dtype)(const int_tp count, const int_tp num,\n                                        const int_tp channels,\n                                        const int_tp spatial_dim,\n                                        __global const Dtype* channel_max,\n                                        __global Dtype* data) {\n  for (int_tp index = get_global_id(0); index < count;\n      index += get_global_size(0)) {\n    int_tp n = index / channels / spatial_dim;\n    int_tp s = index % spatial_dim;\n    data[index] -= channel_max[n * spatial_dim + s];\n  }\n}\n\n__kernel void TEMPLATE(kernel_exp,Dtype)(const int_tp count, __global const Dtype* data,\n                           __global Dtype* out) {\n  for (int_tp index = get_global_id(0); index < count;\n      index += get_global_size(0)) {\n    out[index] = exp(data[index]);\n  }\n}\n\n__kernel void TEMPLATE(kernel_channel_sum,Dtype)(const int_tp num, const int_tp channels,\n                                   const int_tp spatial_dim,\n                                   __global const Dtype* data,\n                                   __global Dtype* channel_sum) {\n  for (int_tp index = get_global_id(0); index < num * spatial_dim; index +=\n      get_global_size(0)) {\n    int_tp n = index / spatial_dim;\n    int_tp s = index % spatial_dim;\n    Dtype sum = 0;\n    for (int_tp c = 0; c < channels; ++c) {\n      sum += data[(n * channels + c) * spatial_dim + s];\n    }\n    channel_sum[index] = sum;\n  }\n}\n\n__kernel void TEMPLATE(kernel_channel_div,Dtype)(const int_tp count, const int_tp num,\n                                   const int_tp channels, const int_tp spatial_dim,\n                                   __global const Dtype* channel_sum,\n                                   __global Dtype* data) {\n  for (int_tp index = get_global_id(0); index < count;\n      index += get_global_size(0)) {\n    int_tp n = index / channels / spatial_dim;\n    int_tp s = index % spatial_dim;\n    data[index] /= channel_sum[n * spatial_dim + s];\n  }\n}\n\n__kernel void TEMPLATE(kernel_channel_dot,Dtype)(const int_tp num, const int_tp channels,\n                                   const int_tp spatial_dim,\n                                   __global const Dtype* data_1,\n                                   __global const Dtype* data_2,\n                                   __global Dtype* channel_dot) {\n  for (int_tp index = get_global_id(0); index < num * spatial_dim; index +=\n      get_global_size(0)) {\n    int_tp n = index / spatial_dim;\n    int_tp s = index % spatial_dim;\n    Dtype dot = 0;\n    for (int_tp c = 0; c < channels; ++c) {\n      dot += (data_1[(n * channels + c) * spatial_dim + s]\n          * data_2[(n * channels + c) * spatial_dim + s]);\n    }\n    channel_dot[index] = dot;\n  }\n}";  // NOLINT
std::string concat_float = "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(concat,Dtype)(const int_tp nthreads, __global const Dtype* in_data,\n                                     const int forward, const int_tp num_concats,\n                                     const int_tp concat_size,\n                                     const int_tp top_concat_axis,\n                                     const int_tp bottom_concat_axis,\n                                     const int_tp offset_concat_axis,\n                                     __global Dtype* out_data) {\n\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    const int_tp total_concat_size = concat_size * bottom_concat_axis;\n    const int_tp concat_num = index / total_concat_size;\n    const int_tp concat_index = index % total_concat_size;\n    const int_tp top_index = concat_index\n        + (concat_num * top_concat_axis + offset_concat_axis) * concat_size;\n    if (forward == 1) {\n      out_data[top_index] = in_data[index];\n    } else {\n      out_data[index] = in_data[top_index];\n    }\n  }\n}";  // NOLINT
std::string contrastive_loss_float = "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(cll_backward,Dtype)(const int_tp count, const int_tp channels,\n                            const Dtype margin, const int legacy_version,\n                            const Dtype alpha, __global const Dtype* y,\n                            __global const Dtype* diff, __global const Dtype* dist_sq,\n                            __global Dtype *bottom_diff) {\n  for (int_tp i = get_global_id(0); i < count;\n      i += get_global_size(0)) {\n    int_tp n = i / channels;  // the num index, to access y and dist_sq\n    if ((int_tp)(y[n])) {  // similar pairs\n      bottom_diff[i] = alpha * diff[i];\n    } else {  // dissimilar pairs\n      Dtype mdist = 0.0;\n      Dtype beta = 0.0;\n      if (legacy_version == 1) {\n        mdist = (margin - dist_sq[n]);\n        beta = -alpha;\n      } else {\n        Dtype dist = sqrt(dist_sq[n]);\n        mdist = (margin - dist);\n        beta = -alpha * mdist / (dist + 1e-4) * diff[i];\n      }\n      if (mdist > 0.0) {\n        bottom_diff[i] = beta;\n      } else {\n        bottom_diff[i] = 0;\n      }\n    }\n  }\n}";  // NOLINT
std::string dropout_float = "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(dropout_forward,Dtype)(const int_tp n,\n                                              __global const Dtype* in,\n                                              __global const uint_tp* mask,\n                                              const uint_tp threshold,\n                                              const Dtype scale,\n                                              __global Dtype* out) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    out[index] = in[index] * ((Dtype)(mask[index] > threshold)) * scale;\n  }\n}\n\n__kernel void TEMPLATE(dropout_backward,Dtype)(\n    const int_tp n, __global const Dtype* in_diff,\n    __global const uint_tp* mask, const uint_tp threshold,\n    const Dtype scale,\n    __global Dtype* out_diff) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    out_diff[index] = in_diff[index] * scale * ((Dtype)(mask[index] > threshold));\n  }\n}";  // NOLINT
std::string eltwise_float = "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(eltwise_max_forward,Dtype)(\n    const int_tp nthreads, __global const Dtype* bottom_data_a,\n    __global const Dtype* bottom_data_b, const int_tp blob_idx,\n    __global Dtype* top_data,\n    __global int_tp* mask) {\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    Dtype maxval = -FLT_MAX;\n    int_tp maxidx = -1;\n    if (bottom_data_a[index] > bottom_data_b[index]) {\n      // only update for very first bottom_data blob (blob_idx == 0)\n      if (blob_idx == 0) {\n        maxval = bottom_data_a[index];\n        top_data[index] = maxval;\n        maxidx = blob_idx;\n        mask[index] = maxidx;\n      }\n    } else {\n      maxval = bottom_data_b[index];\n      top_data[index] = maxval;\n      maxidx = blob_idx + 1;\n      mask[index] = maxidx;\n    }\n  }\n}\n\n__kernel void TEMPLATE(eltwise_max_backward,Dtype)(const int_tp nthreads,\n                                                   __global const Dtype* top_diff,\n                                                   const int_tp blob_idx,\n                                                   __global const int_tp* mask,\n                                                   __global Dtype* bottom_diff) {\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    Dtype gradient = 0;\n    if (mask[index] == blob_idx) {\n      gradient += top_diff[index];\n    }\n    bottom_diff[index] = gradient;\n  }\n}";  // NOLINT
std::string embed_float = "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(embed_forward,Dtype)(const int_tp nthreads,\n                                            __global const Dtype* bottom_data,\n                                            __global const Dtype* weight,\n                                            const int_tp M, const int_tp N,\n                                            const int_tp K,\n                                            __global Dtype* top_data) {\n  for (int_tp top_index = get_global_id(0); top_index < nthreads;\n      top_index += get_global_size(0)) {\n      const int_tp n = top_index / N;\n      const int_tp d = top_index % N;\n      const int_tp index = (int_tp)(bottom_data[n]);\n      const int_tp weight_index = index * N + d;\n      top_data[top_index] = weight[weight_index];\n    }\n  }\n\n// atomic_add from: http://suhorukov.blogspot.com/2011/12/opencl-11-atomic-operations-on-floating.html\n#if (TYPE == TYPE_FLOAT)\ninline void TEMPLATE(atomic_add,Dtype)(volatile __global Dtype *source, const Dtype operand) {\n    union {\n        uint_tp intVal;\n        Dtype floatVal;\n    } newVal;\n    union {\n        uint_tp intVal;\n        Dtype floatVal;\n    } prevVal;\n    do {\n        prevVal.floatVal = *source;\n        newVal.floatVal = prevVal.floatVal + operand;\n    } while (atomic_cmpxchg((volatile __global unsigned int *)source, prevVal.intVal, newVal.intVal) != prevVal.intVal);\n}\n\n__kernel void TEMPLATE(embed_backward,Dtype)(const int_tp nthreads, __global const Dtype* bottom_data,\n    __global const Dtype* top_diff, const int_tp M, const int_tp N, const int_tp K,\n    __global Dtype* weight_diff) {\n  for (int_tp top_index = get_global_id(0); top_index < nthreads;\n      top_index += get_global_size(0)) {\n    const int_tp n = top_index / N;\n    const int_tp d = top_index % N;\n    const int_tp index = (int_tp)(bottom_data[n]);\n    const int_tp weight_index = index * N + d;\n\n    TEMPLATE(atomic_add,Dtype)((weight_diff + weight_index), *(top_diff + top_index));\n  }\n}\n#endif\n\n#if (TYPE == TYPE_DOUBLE)\n#ifdef ATOMICS_64_AVAILABLE\ninline void TEMPLATE(atomic_add,Dtype)(volatile __global Dtype *source, const Dtype operand) {\n    union {\n        unsigned long intVal;\n        Dtype floatVal;\n    } newVal;\n    union {\n        unsigned long intVal;\n        Dtype floatVal;\n    } prevVal;\n    do {\n        prevVal.floatVal = *source;\n        newVal.floatVal = prevVal.floatVal + operand;\n    } while (atom_cmpxchg((volatile __global unsigned long *)source, prevVal.intVal, newVal.intVal) != prevVal.intVal);\n}\n\n__kernel void TEMPLATE(embed_backward,Dtype)(const int_tp nthreads, __global const Dtype* bottom_data,\n    __global const Dtype* top_diff, const int_tp M, const int_tp N, const int_tp K,\n    __global Dtype* weight_diff) {\n  for (int_tp top_index = get_global_id(0); top_index < nthreads;\n      top_index += get_global_size(0)) {\n    const int_tp n = top_index / N;\n    const int_tp d = top_index % N;\n    const int_tp index = (int_tp)(bottom_data[n]);\n    const int_tp weight_index = index * N + d;\n\n    TEMPLATE(atomic_add,Dtype)((weight_diff + weight_index), *(top_diff + top_index));\n  }\n}\n#endif\n#endif";  // NOLINT
std::string fillbuffer_float = "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(fillbuffer,Dtype)(const int_tp n, const char alpha, __global char* x,\n                                   const int_tp offx) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    x[index + offx] = alpha;\n  }\n}\n\n__kernel void TEMPLATE(fill,Dtype)(const int_tp n, const Dtype alpha, __global Dtype* x,\n                                   const int_tp offx) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    x[index + offx] = alpha;\n  }\n}";  // NOLINT
std::string im2col_float = "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(im2col,Dtype)(const int_tp n, __global const Dtype* data_im, const int_tp data_im_off,\n    const int_tp height, const int_tp width, const int_tp kernel_h, const int_tp kernel_w,\n    const int_tp pad_h, const int_tp pad_w,\n    const int_tp stride_h, const int_tp stride_w,\n    const int_tp height_col, const int_tp width_col,\n    __global Dtype* data_col, const int_tp data_col_off) {\n\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    int_tp w_out = index % width_col;\n    int_tp h_index = index / width_col;\n    int_tp h_out = h_index % height_col;\n    int_tp channel_in = h_index / height_col;\n    int_tp channel_out = channel_in * kernel_h * kernel_w;\n    int_tp h_in = h_out * stride_h - pad_h;\n    int_tp w_in = w_out * stride_w - pad_w;\n    __global Dtype* data_col_ptr = data_col + data_col_off;\n    data_col_ptr += (channel_out * height_col + h_out) * width_col + w_out;\n    __global const Dtype* data_im_ptr = data_im + data_im_off;\n    data_im_ptr += (channel_in * height + h_in) * width + w_in;\n    for (int_tp i = 0; i < kernel_h; ++i) {\n      for (int_tp j = 0; j < kernel_w; ++j) {\n        int_tp h = h_in + i;\n        int_tp w = w_in + j;\n        *data_col_ptr = (h >= 0 && w >= 0 && h < height && w < width) ?\n            data_im_ptr[i * width + j] : 0;\n        data_col_ptr += height_col * width_col;\n      }\n    }\n  }\n}\n\n__kernel void TEMPLATE(col2im,Dtype)(const int_tp n, __global const Dtype* data_col, const int_tp data_col_off,\n    const int_tp height, const int_tp width, const int_tp channels,\n    const int_tp patch_h, const int_tp patch_w,\n    const int_tp pad_h, const int_tp pad_w,\n    const int_tp stride_h, const int_tp stride_w,\n    const int_tp height_col, const int_tp width_col,\n    __global Dtype* data_im, const int_tp data_im_off) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    Dtype val = 0;\n    int_tp w = index % width + pad_w;\n    int_tp h = (index / width) % height + pad_h;\n    int_tp c = index / (width * height);\n    // compute the start and end of the output\n    int_tp w_col_start = (w < patch_w) ? 0 : (w - patch_w) / stride_w + 1;\n    int_tp w_col_end = min(w / stride_w + 1, width_col);\n    int_tp h_col_start = (h < patch_h) ? 0 : (h - patch_h) / stride_h + 1;\n    int_tp h_col_end = min(h / stride_h + 1, height_col);\n    int_tp offset = data_col_off +\n        (c * patch_h * patch_w + h * patch_w + w) * height_col * width_col;\n    int_tp coeff_h_col = (1 - stride_h * patch_w * height_col) * width_col;\n    int_tp coeff_w_col = (1 - stride_w * height_col * width_col);\n    for (int_tp h_col = h_col_start; h_col < h_col_end; ++h_col) {\n      for (int_tp w_col = w_col_start; w_col < w_col_end; ++w_col) {\n        val += data_col[offset + h_col * coeff_h_col + w_col * coeff_w_col];\n      }\n    }\n    data_im[index + data_im_off] = val;\n  }\n}";  // NOLINT
std::string im2col_nd_float = "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(im2col_nd, Dtype)(const int_tp n, const int_tp num_axes,\n                                     const int_tp channel_axis,\n                                     __global const Dtype* data_im,\n                                     const int_tp data_off,\n                                     __global const int_tp* im_shape,\n                                     __global const int_tp* col_shape,\n                                     __global const int_tp* kernel_shape,\n                                     __global const int_tp* pad,\n                                     __global const int_tp* stride,\n                                     __global Dtype* data_col,\n                                     const int_tp data_col_off) {\n\n  int_tp d_temp[6];\n  int_tp d_iter[6];\n  int_tp i;\n\n  __global const int_tp* im_shape_ptr = im_shape + channel_axis;\n  __global const int_tp* col_shape_ptr = col_shape + channel_axis;\n\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    // Initialize channel_in, computed in the loop below, with intermediate\n    // computations used to compute the spatial indices.\n    int_tp channel_in = index;\n    int_tp channel_out = 1;\n    for (i = num_axes - 1; i >= 0; --i) {\n      d_temp[i] = channel_in % col_shape_ptr[i + 1];\n      channel_in /= col_shape_ptr[i + 1];\n      channel_out *= kernel_shape[i];\n    }\n    channel_out *= channel_in;\n    int_tp data_col_inc = 1;\n    for (i = 0; i < num_axes; ++i) {\n      channel_out *= col_shape_ptr[i + 1];\n      channel_out += d_temp[i];\n      d_temp[i] = d_temp[i] * stride[i] - pad[i];\n      channel_in *= im_shape_ptr[i + 1];\n      channel_in += d_temp[i];\n      data_col_inc *= col_shape_ptr[i + 1];\n      d_iter[i] = 0;\n    }\n    __global Dtype* data_col_ptr = data_col + data_col_off + channel_out;\n    __global const Dtype* data_im_ptr = data_im + data_off + channel_in;\n    bool incremented;\n    do {\n      bool in_range = true;\n      for (i = 0; i < num_axes; ++i) {\n        const int_tp d_iter_im = d_iter[i] + d_temp[i];\n        in_range &= d_iter_im >= 0 && d_iter_im < im_shape_ptr[i + 1];\n        if (!in_range) {\n          break;\n        }\n      }\n      if (in_range) {\n        int_tp data_im_offset = d_iter[0];\n        for (i = 1; i < num_axes; ++i) {\n          data_im_offset *= im_shape_ptr[i + 1];\n          data_im_offset += d_iter[i];\n        }\n        *data_col_ptr = data_im_ptr[data_im_offset];\n      } else {\n        *data_col_ptr = 0;\n      }\n      data_col_ptr += data_col_inc;\n      incremented = false;\n      for (i = num_axes - 1; i >= 0; --i) {\n        const int_tp d_max = kernel_shape[i];\n        if (d_iter[i] == d_max - 1) {\n          d_iter[i] = 0;\n        } else {  // d_iter[i] < d_max - 1\n          ++d_iter[i];\n          incremented = true;\n          break;\n        }\n      }  // for (int_tp i = num_axes - 1; i >= 0; --i)\n    } while (incremented);  // do\n  }\n}\n\n\n\n__kernel void TEMPLATE(col2im_nd, Dtype)(const int_tp n, const int_tp num_axes,\n                                         const int_tp channel_axis,\n                                         __global const Dtype* data_col,\n                                         const int_tp data_col_off,\n                                         __global const int_tp* im_shape,\n                                         __global const int_tp* col_shape,\n                                         __global const int_tp* kernel_shape,\n                                         __global const int_tp* pad,\n                                         __global const int_tp* stride,\n                                         __global Dtype* data_im,\n                                         const int_tp data_im_off) {\n  int_tp d_im[6];\n  int_tp d_col_iter[6];\n  int_tp d_col_start[6];\n  int_tp d_col_end[6];\n\n  __global const int_tp* im_shape_ptr = im_shape + channel_axis;\n  __global const int_tp* col_shape_ptr = col_shape + channel_axis;\n  __global Dtype* data_col_ptr = data_col + data_col_off;\n\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    // Initialize channel_in, computed in the loop below, with intermediate\n    // computations used to compute the spatial indices.\n    int_tp channel_im = index;\n    // Calculate d_im (image dimensions).\n    for (int_tp i = num_axes - 1; i >= 0; --i) {\n      d_im[i] = channel_im % im_shape_ptr[i + 1] + pad[i];\n      channel_im /= im_shape_ptr[i + 1];\n    }\n    // Calculate col start/end indices.\n    bool done = false;\n    for (int_tp i = 0; i < num_axes; ++i) {\n      d_col_start[i] = d_col_iter[i] =\n          (d_im[i] < kernel_shape[i]) ?\n              0 : (d_im[i] - kernel_shape[i]) / stride[i] + 1;\n      d_col_end[i] = min(d_im[i] / stride[i] + 1, col_shape_ptr[i + 1]);\n      if (d_col_start[i] >= d_col_end[i]) {\n        // Skip computation if the dimension is 0 at any spatial axis --\n        // final val will be 0.\n        data_im[index + data_im_off] = 0;\n        done = true;\n        break;  // for (int_tp i = 0; i < num_axes; ++i)\n      }\n    }\n    if (done) {\n      continue;\n    }\n    // Loop over the col to compute the output val.\n    Dtype val = 0;\n    bool incremented = true;\n    do {\n      // Compute the final offset.\n      int_tp final_offset = 0;\n      int_tp kernel_shape_prod = 1;\n      for (int_tp i = num_axes - 1; i >= 0; --i) {\n        final_offset += (d_im[i] - d_col_iter[i] * stride[i])\n            * kernel_shape_prod;\n        kernel_shape_prod *= kernel_shape[i];\n      }\n      final_offset += kernel_shape_prod * channel_im;\n      for (int_tp i = 0; i < num_axes; ++i) {\n        final_offset *= col_shape_ptr[i + 1];\n        final_offset += d_col_iter[i];\n      }\n      val += data_col_ptr[final_offset];\n      incremented = false;\n      for (int_tp i = num_axes - 1; i >= 0; --i) {\n        const int_tp d_max = d_col_end[i];\n        if (d_col_iter[i] == d_max - 1) {\n          d_col_iter[i] = d_col_start[i];\n        } else {  // d_col_iter[i] < d_max - 1\n          ++d_col_iter[i];\n          incremented = true;\n          break;  // for (int_tp i = num_axes - 1; i >= 0; --i)\n        }\n      }  // for (int_tp i = num_axes - 1; i >= 0; --i)\n    } while (incremented);\n    data_im[index + data_im_off] = val;\n  }\n}";  // NOLINT
std::string im2col_ndsk_float = "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(im2col_ndsk, Dtype)(const int_tp n, const int_tp num_axes,\n                                        __global const Dtype* data_im,\n                                        const int_tp data_off,\n                                        __global const int_tp* im_shape,\n                                        __global const int_tp* col_shape,\n                                        __global const int_tp* kernel_shape,\n                                        __global const int_tp* pad,\n                                        __global const int_tp* stride,\n                                        __global const int_tp* kstride,\n                                        __global Dtype* data_col,\n                                        const int_tp data_col_off) {\n  int_tp d_temp[6];\n  int_tp d_iter[6];\n  int_tp i;\n\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    // Initialize channel_in, computed in the loop below, with intermediate\n    // computations used to compute the spatial indices.\n    int_tp channel_in = index;\n    int_tp channel_out = 1;\n    for (i = num_axes - 1; i >= 0; --i) {\n      d_temp[i] = channel_in % col_shape[i + 1];\n      channel_in /= col_shape[i + 1];\n      channel_out *= kernel_shape[i];\n    }\n    channel_out *= channel_in;\n    int_tp data_col_inc = 1;\n    for (i = 0; i < num_axes; ++i) {\n      channel_out *= col_shape[i + 1];\n      channel_out += d_temp[i];\n      d_temp[i] = d_temp[i] * stride[i] - pad[i];\n      channel_in *= im_shape[i + 1];\n      channel_in += d_temp[i];\n      data_col_inc *= col_shape[i + 1];\n      d_iter[i] = 0;\n    }\n    __global Dtype* data_col_ptr = data_col + data_col_off + channel_out;\n    __global const Dtype* data_im_ptr = data_im + data_off + channel_in;\n    bool incremented;\n    do {\n      bool in_range = true;\n      for (i = 0; i < num_axes; ++i) {\n        const int_tp d_iter_im = d_iter[i] + d_temp[i];\n        in_range &= d_iter_im >= 0 && d_iter_im < im_shape[i + 1];\n        if (!in_range) {\n          break;\n        }\n      }\n\n      // Write column data\n      if (in_range) {\n        int_tp data_im_offset = d_iter[0];\n        for (i = 1; i < num_axes; ++i) {\n          data_im_offset *= im_shape[i + 1];\n          data_im_offset += d_iter[i];\n        }\n        *data_col_ptr = data_im_ptr[data_im_offset];\n      } else {\n        *data_col_ptr = 0;\n      }\n\n      data_col_ptr += data_col_inc;\n      incremented = false;\n      for (i = num_axes - 1; i >= 0; --i) {\n        // Old: const int_tp d_max = kernel_shape[i];\n        // New (strided, limit is the external kernel size):\n        const int_tp d_max = (kernel_shape[i] - 1) * kstride[i] + 1;\n        if (d_iter[i] == d_max - 1) {\n          d_iter[i] = 0;\n        } else {  // d_iter[i] < d_max - 1\n          // Old: ++d_iter[i];\n          // New (strided, increment by the stride each time):\n          d_iter[i] += kstride[i];\n          incremented = true;\n          break;\n        }\n      }  // for (int_tp i = num_axes - 1; i >= 0; --i)\n    } while (incremented);  // do\n  }\n}\n\n__kernel void TEMPLATE(col2im_ndsk, Dtype)(const int_tp n, const int_tp num_axes,\n                                  __global const Dtype* data_col,\n                                    const int_tp data_col_off,\n                                  __global const int_tp* im_shape,\n                                  __global const int_tp* col_shape,\n                                  __global const int_tp* kernel_shape,\n                                  __global const int_tp* pad,\n                                  __global const int_tp* stride,\n                                  __global const int_tp* kstride,\n                                  __global Dtype* data_im,\n                                  const int_tp data_off) {\n  int_tp d_im[6];\n  int_tp d_col_size[6];\n  int_tp d_col_iter[6];\n  int_tp d_col_start[6];\n  int_tp d_col_end[6];\n  int_tp d_ext_patch[6];\n  int_tp d_idx[6];\n\n  for (int_tp i = num_axes - 1; i >= 0; --i) {\n    d_ext_patch[i] = (kernel_shape[i] - 1) * kstride[i] + 1;\n    d_col_size[i] = (im_shape[i + 1] + 2 * pad[i] - d_ext_patch[i])\n        / stride[i] + 1;\n  }\n\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    // Initialize channel_in, computed in the loop below, with intermediate\n    // computations used to compute the spatial indices.\n    int_tp channel_im = index;\n    // Calculate d_im (image dimensions).\n    for (int_tp i = num_axes - 1; i >= 0; --i) {\n      d_im[i] = channel_im % im_shape[i + 1] + pad[i];\n      channel_im /= im_shape[i + 1];\n    }\n    // Calculate col start/end indices.\n    bool done = false;\n    for (int_tp i = 0; i < num_axes; ++i) {\n      // Old:\n      /*d_col_start[i] = d_col_iter[i] =\n          (d_im[i] < kernel_shape[i]) ?\n          0 : (d_im[i] - kernel_shape[i]) / stride[i] + 1;\n      d_col_end[i] = min(d_im[i] / stride[i] + 1, col_shape[i + 1]);*/\n      // New:\n      d_col_start[i] = (d_im[i] < d_ext_patch[i]) ?\n          d_im[i] % kstride[i] : (d_im[i] - d_ext_patch[i]) + 1;\n      d_col_iter[i] = d_col_start[i];\n      d_idx[i] = (d_im[i] - d_col_start[i]) / kstride[i];\n      d_col_end[i] = (d_im[i] >= d_col_size[i]) ?\n          (d_col_size[i] - 1) - ((d_col_size[i] - 1) - d_col_start[i])\n          % kstride[i] : d_im[i];\n      if (d_col_start[i] > d_col_end[i]) {\n        // Skip computation if the dimension is 0 at any spatial axis --\n        // final val will be 0.\n        data_im[index] = 0;\n        done = true;\n        break;  // for (int_tp i = 0; i < num_axes; ++i)\n      }\n    }\n    if (done) {\n      continue;\n    }\n    // Loop over the col to compute the output val.\n    Dtype val = 0;\n    bool incremented = true;\n    do {\n      // Compute the final offset.\n      int_tp final_offset = 0;\n      int_tp coeff_prod = 1;\n      for (int_tp i = num_axes - 1; i >= 0; --i) {\n        final_offset +=  d_col_iter[i] * coeff_prod;\n        coeff_prod *= d_col_size[i];\n      }\n      for (int_tp i = num_axes - 1; i >= 0; --i) {\n        final_offset += d_idx[i] * coeff_prod;\n        coeff_prod *= kernel_shape[i];\n      }\n      final_offset += channel_im * coeff_prod;\n      val += data_col[final_offset];\n      incremented = false;\n      for (int_tp i = num_axes - 1; i >= 0; --i) {\n        if (d_col_iter[i] > d_col_end[i] - kstride[i]) {\n          d_col_iter[i] = d_col_start[i];\n          d_idx[i] = (d_im[i] - d_col_start[i]) / kstride[i];\n        } else {  // d_col_iter[i] <= d_max - kstride[1]\n          d_col_iter[i] += kstride[i];\n          --d_idx[i];\n          incremented = true;\n          break;  // for (int_tp i = num_axes - 1; i >= 0; --i)\n        }\n      }  // for (int_tp i = num_axes - 1; i >= 0; --i)\n    }  while (incremented);\n    data_im[index] = val;\n  }\n}";  // NOLINT
std::string im2col_sk_float = "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(im2col_sk,Dtype)(const int_tp n,\n                                        __global const Dtype* data_im,\n                                        const int_tp data_offset, const int_tp height,\n                                        const int_tp width, const int_tp kernel_h,\n                                        const int_tp kernel_w,\n                                        const int_tp ext_kernel_h,\n                                        const int_tp ext_kernel_w, const int_tp pad_h,\n                                        const int_tp pad_w, const int_tp stride_h,\n                                        const int_tp stride_w, const int_tp kstride_h,\n                                        const int_tp kstride_w,\n                                        const int_tp height_col,\n                                        const int_tp width_col,\n                                        __global Dtype* data_col) {\n\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    int_tp w_out = index % width_col;\n    int_tp h_index = index / width_col;\n    int_tp h_out = h_index % height_col;\n    int_tp channel_in = h_index / height_col;\n    int_tp channel_out = channel_in * kernel_h * kernel_w;\n    int_tp h_in = h_out * stride_h - pad_h;\n    int_tp w_in = w_out * stride_w - pad_w;\n    __global Dtype* data_col_ptr = data_col;\n    data_col_ptr += (channel_out * height_col + h_out) * width_col + w_out;\n    __global const Dtype* data_im_ptr = data_im + data_offset;\n    data_im_ptr += (channel_in * height + h_in) * width + w_in;\n    for (int_tp i = 0; i < ext_kernel_h; i += kstride_h) {\n      for (int_tp j = 0; j < ext_kernel_w; j += kstride_w) {\n        int_tp h = h_in + i;\n        int_tp w = w_in + j;\n        (*data_col_ptr) =\n            (h >= 0 && w >= 0 && h < height && w < width) ?\n                data_im_ptr[i * width + j] : 0;\n        data_col_ptr += height_col * width_col;\n      }\n    }\n  }\n\n}\n\n__kernel void TEMPLATE(col2im_sk,Dtype)(const int_tp n,\n                                        __global const Dtype* data_col,\n                                        const int_tp height, const int_tp width,\n                                        const int_tp channels, const int_tp patch_h,\n                                        const int_tp patch_w,\n                                        const int_tp ext_patch_h,\n                                        const int_tp ext_patch_w, const int_tp pad_h,\n                                        const int_tp pad_w, const int_tp stride_h,\n                                        const int_tp stride_w, const int_tp kstride_h,\n                                        const int_tp kstride_w,\n                                        const int_tp height_col,\n                                        const int_tp width_col,\n                                        __global Dtype* data_im,\n                                        const int_tp data_offset) {\n\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    Dtype val = 0;\n    int_tp w = index % width + pad_w;\n    int_tp h = (index / width) % height + pad_h;\n    int_tp c = index / (width * height);\n    // compute the start and end of the output\n    int_tp width_col_1 = width_col - 1;\n    int_tp height_col_1 = height_col - 1;\n    int_tp w_col_start = (w < ext_patch_w) ? w % kstride_w : (w - ext_patch_w) + 1;\n    int_tp w_col_end =\n        (w >= width_col) ?\n            width_col_1 - (width_col_1 - w_col_start) % kstride_w : w;\n    int_tp h_col_start = (h < ext_patch_h) ? h % kstride_h : (h - ext_patch_h) + 1;\n    int_tp h_col_end =\n        (h >= height_col) ?\n            height_col_1 - (height_col_1 - h_col_start) % kstride_h : h;\n    int_tp w_num = (w - w_col_start) / kstride_w;\n    int_tp h_num = (h - h_col_start) / kstride_h;\n\n    int_tp coeff_w_idx = height_col * width_col;\n    int_tp coeff_h_idx = patch_w * coeff_w_idx;\n    int_tp offset = c * patch_h * coeff_h_idx;\n    for (int_tp h_col = h_col_start, h_idx = h_num; h_col <= h_col_end; h_col +=\n        kstride_h, --h_idx) {\n      for (int_tp w_col = w_col_start, w_idx = w_num; w_col <= w_col_end; w_col +=\n          kstride_w, --w_idx) {\n        //int_tp c_col = c * patch_h * patch_w + (h - h_col) / kstride_h * patch_w + (w - w_col) / kstride_w;\n        //int_tp c_col = c * patch_h * patch_w + h_idx * patch_w + w_idx;\n        //val += data_col[(c_col * height_col + h_col) * width_col + w_col];\n        val += data_col[offset + h_idx * coeff_h_idx + w_idx * coeff_w_idx\n            + h_col * width_col + w_col];\n      }\n    }\n\n    data_im[data_offset + index] = val;\n  }\n}";  // NOLINT
std::string lrn_float = "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(lrn_compute_output,Dtype)(const int_tp nthreads,\n                                                 __global const Dtype* in,\n                                                 __global const Dtype* scale,\n                                                 const Dtype negative_beta,\n                                                 __global Dtype* out) {\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    out[index] = in[index] * pow(scale[index], negative_beta);\n  }\n}\n\n__kernel void TEMPLATE(lrn_fill_scale,Dtype)(const int_tp nthreads, __global const Dtype* in,\n                             const int_tp num, const int_tp channels,\n                             const int_tp height, const int_tp width, const int_tp size,\n                             const Dtype alpha_over_size, const Dtype k,\n                             __global Dtype* const scale) {\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    // find out the local offset\n    const int_tp w = index % width;\n    const int_tp h = (index / width) % height;\n    const int_tp n = index / width / height;\n    const int_tp offset = (n * channels * height + h) * width + w;\n    const int_tp step = height * width;\n    __global const Dtype* in_off = in + offset;\n    __global Dtype* scale_off = scale + offset;\n    int_tp head = 0;\n    const int_tp pre_pad = (size - 1) / 2;\n    const int_tp post_pad = size - pre_pad - 1;\n    Dtype accum_scale = 0;\n    // fill the scale at [n, :, h, w]\n    // accumulate values\n    while (head < post_pad && head < channels) {\n      accum_scale += in_off[head * step] * in_off[head * step];\n      ++head;\n    }\n    // both add and subtract\n    while (head < channels) {\n      accum_scale += in_off[head * step] * in_off[head * step];\n      if (head - size >= 0) {\n        accum_scale -= in_off[(head - size) * step]\n            * in_off[(head - size) * step];\n      }\n      scale_off[(head - post_pad) * step] = k + accum_scale * alpha_over_size;\n      ++head;\n    }\n    // subtract only\n    while (head < channels + post_pad) {\n      if (head - size >= 0) {\n        accum_scale -= in_off[(head - size) * step]\n            * in_off[(head - size) * step];\n      }\n      scale_off[(head - post_pad) * step] = k + accum_scale * alpha_over_size;\n      ++head;\n    }\n  }\n}\n\n__kernel void TEMPLATE(lrn_compute_diff,Dtype)(const int_tp nthreads,\n                               __global const Dtype* bottom_data,\n                               __global const Dtype* top_data,\n                               __global const Dtype* scale,\n                               __global const Dtype* top_diff, const int_tp num,\n                               const int_tp channels, const int_tp height,\n                               const int_tp width, const int_tp size,\n                               const Dtype negative_beta,\n                               const Dtype cache_ratio,\n                               __global Dtype* bottom_diff) {\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    // find out the local offset\n    const int_tp w = index % width;\n    const int_tp h = (index / width) % height;\n    const int_tp n = index / width / height;\n    const int_tp offset = (n * channels * height + h) * width + w;\n    const int_tp step = height * width;\n    __global const Dtype* bottom_off = bottom_data + offset;\n    __global const Dtype* top_off = top_data + offset;\n    __global const Dtype* scale_off = scale + offset;\n    __global Dtype* top_diff_off = top_diff + offset;\n    __global Dtype* bottom_diff_off = bottom_diff + offset;\n    int_tp head = 0;\n    const int_tp pre_pad = size - (size + 1) / 2;\n    const int_tp post_pad = size - pre_pad - 1;\n    Dtype accum_ratio = 0;\n    // accumulate values\n    while (head < post_pad && head < channels) {\n      accum_ratio += top_diff_off[head * step] * top_off[head * step]\n          / scale_off[head * step];\n      ++head;\n    }\n    // both add and subtract\n    while (head < channels) {\n      accum_ratio += top_diff_off[head * step] * top_off[head * step]\n          / scale_off[head * step];\n      if (head - size >= 0) {\n        accum_ratio -= top_diff_off[(head - size) * step]\n            * top_off[(head - size) * step] / scale_off[(head - size) * step];\n      }\n      bottom_diff_off[(head - post_pad) * step] = top_diff_off[(head - post_pad)\n          * step] * pow(scale_off[(head - post_pad) * step], negative_beta)\n          - cache_ratio * bottom_off[(head - post_pad) * step] * accum_ratio;\n      ++head;\n    }\n    // subtract only\n    while (head < channels + post_pad) {\n      if (head - size >= 0) {\n        accum_ratio -= top_diff_off[(head - size) * step]\n            * top_off[(head - size) * step] / scale_off[(head - size) * step];\n      }\n      bottom_diff_off[(head - post_pad) * step] = top_diff_off[(head - post_pad)\n          * step] * pow(scale_off[(head - post_pad) * step], negative_beta)\n          - cache_ratio * bottom_off[(head - post_pad) * step] * accum_ratio;\n      ++head;\n    }\n  }\n}";  // NOLINT
std::string math_float = "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(mul,Dtype)(const int_tp n, __global const Dtype* a,\n                                  const int_tp offa,\n                                  __global Dtype* b,\n                                  const int_tp offb, __global Dtype* y,\n                                  const int_tp offy) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    y[index + offy] = a[index + offa] * b[index + offb];\n  }\n}\n\n__kernel void TEMPLATE(div,Dtype)(const int_tp n, __global const Dtype* a,\n                                  const int_tp offa,\n                                  __global Dtype* b,\n                                  const int_tp offb, __global Dtype* y,\n                                  const int_tp offy) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    y[index + offy] = a[index + offa] / b[index + offb];\n  }\n}\n\n__kernel void TEMPLATE(add_scalar,Dtype)(const int_tp N, const Dtype alpha,\n__global Dtype* Y,\n                                         const int_tp offY) {\n  for (int_tp index = get_global_id(0); index < N; index += get_global_size(0)) {\n    Y[offY + index] += alpha;\n  }\n}\n\n__kernel void TEMPLATE(add,Dtype)(const int_tp n, __global const Dtype* a,\n                                  const int_tp offa, __global const Dtype* b,\n                                  const int_tp offb, __global Dtype* y,\n                                  const int_tp offy) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    y[offy + index] = a[offa + index] + b[offb + index];\n  }\n}\n\n__kernel void TEMPLATE(sub,Dtype)(const int_tp n, __global const Dtype* a,\n                                  const int_tp offa, __global const Dtype* b,\n                                  const int_tp offb, __global Dtype* y,\n                                  const int_tp offy) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    y[offy + index] = a[offa + index] - b[offb + index];\n  }\n}\n\n__kernel void TEMPLATE(abs,Dtype)(const int_tp n, __global const Dtype* a,\n                                  const int_tp offa, __global Dtype* y,\n                                  const int_tp offy) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    y[offy + index] = fabs((Dtype)(a[offa + index]));\n  }\n}\n\n__kernel void TEMPLATE(exp,Dtype)(const int_tp n, __global const Dtype* a,\n                                  const int_tp offa, __global Dtype* y,\n                                  const int_tp offy) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    y[offy + index] = exp(a[offa + index]);\n  }\n}\n\n__kernel void TEMPLATE(log,Dtype)(const int_tp n, __global const Dtype* a,\n                                  const int_tp offa, __global Dtype* y,\n                                  const int_tp offy) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    y[offy + index] = log(a[offa + index]);\n  }\n}\n\n__kernel void TEMPLATE(powx,Dtype)(const int_tp n, __global const Dtype* a,\n                                   const int_tp offa, Dtype alpha,\n                                   __global Dtype* y,\n                                   const int_tp offy) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    if(alpha == 2.0) {\n      y[offy + index] = pow((Dtype)fabs(a[offa + index]), (Dtype)alpha);\n    } else {\n      y[offy + index] = pow((Dtype)a[offa + index], (Dtype)alpha);\n    }\n  }\n}\n\n__kernel void TEMPLATE(sign,Dtype)(const int_tp n, __global const Dtype* x,\n                                   const int_tp offx, __global Dtype* y,\n                                   const int_tp offy) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    y[index + offy] = (0.0 < x[index + offx])\n        - (x[index + offx] < 0.0);\n  }\n}\n\n__kernel void TEMPLATE(sgnbit,Dtype)(const int_tp n, __global const Dtype* x,\n                                     const int_tp offx, __global Dtype* y,\n                                     const int_tp offy) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    y[index + offy] = signbit(x[index + offx]);\n  }\n}";  // NOLINT
std::string mergecrop_float = "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(merge_copy_forward, Dtype)(const int_tp nthreads,\n                                                  const int_tp dims,\n                                                  __global const Dtype* bottom_a,\n                                                  const int_tp forward_a,\n                                                  __global const Dtype* bottom_b,\n                                                  const int_tp forward_b,\n                                                  __global Dtype* top,\n                                                  const int_tp num,\n                                                  const int_tp channels_a,\n                                                  const int_tp channels_b,\n                                                  __global const int_tp* shape_a,\n                                                  __global const int_tp* shape_b) {\n  int_tp pad[6];\n  int_tp tmp_idx[6];\n  int_tp size_a = 1;\n  int_tp size_b = 1;\n\n  for (int_tp i = 0; i < dims; ++i) {\n    pad[i] = (shape_b[i] - shape_a[i]) / 2;\n    size_a *= shape_a[i];\n    size_b *= shape_b[i];\n  }\n\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    int_tp batch_id = index / ((channels_a + channels_b) * size_a);\n    int_tp bottom_id = ((index - batch_id * (channels_a + channels_b) * size_a)\n        / (channels_a * size_a)) % 2;\n    int_tp counter = index;\n    for (int_tp i = dims - 1; i >= 0; --i) {\n      tmp_idx[i] = counter % shape_a[i];\n      counter /= shape_a[i];\n    }\n\n    if (bottom_id == 0) {\n      int_tp channel_id = (index / size_a) % channels_a;\n      int_tp aidx = batch_id * channels_a + channel_id;\n      for (int_tp i = 0; i < dims; ++i) {\n        aidx *= shape_a[i];\n        aidx += tmp_idx[i];\n      }\n      top[index] = (forward_a == 1) ? bottom_a[aidx] : 0;\n    } else {\n      int_tp channel_id = (index / size_a) % channels_b;\n      int_tp bidx = (batch_id * channels_b + channel_id) * size_b;\n      int_tp btemp = 1;\n      for (int_tp i = dims - 1; i >= 0; --i) {\n        bidx += btemp * (tmp_idx[i] + pad[i]);\n        btemp *= shape_b[i];\n      }\n      top[index] = (forward_b == 1) ? bottom_b[bidx] : 0;\n    }\n  }\n}\n\n__kernel void TEMPLATE(merge_copy_backward,Dtype)(const int_tp nthreads,\n                                                  const int_tp dims,\n                                                  __global Dtype* bottom_a,\n                                                  const int_tp backward_a,\n                                                  __global Dtype* bottom_b,\n                                                  const int_tp backward_b,\n                                                  __global const Dtype* top,\n                                                  const int_tp num,\n                                                  const int_tp channels_a,\n                                                  const int_tp channels_b,\n                                                  __global const int_tp* shape_a,\n                                                  __global const int_tp* shape_b) {\n  int_tp pad[6];\n  int_tp tmp_idx[6];\n  int_tp size_a = 1;\n  int_tp size_b = 1;\n\n  for (int_tp i = 0; i < dims; ++i) {\n    pad[i] = (shape_b[i] - shape_a[i]) / 2;\n    size_a *= shape_a[i];\n    size_b *= shape_b[i];\n  }\n\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    int_tp batch_id = index / ((channels_a + channels_b) * size_a);\n    int_tp bottom_id = ((index - batch_id * (channels_a + channels_b) * size_a)\n        / (channels_a * size_a)) % 2;\n    int_tp counter = index;\n    for (int_tp i = dims - 1; i >= 0; --i) {\n      tmp_idx[i] = counter % shape_a[i];\n      counter /= shape_a[i];\n    }\n\n    if (bottom_id == 0) {\n      int_tp channel_id = (index / size_a) % channels_a;\n      int_tp aidx = batch_id * channels_a + channel_id;\n      for (int_tp i = 0; i < dims; ++i) {\n        aidx *= shape_a[i];\n        aidx += tmp_idx[i];\n      }\n      bottom_a[aidx] = (backward_a == 1) ? top[index] : 0;\n    } else {\n      int_tp channel_id = (index / size_a) % channels_b;\n      int_tp bidx = (batch_id * channels_b + channel_id) * size_b;\n      int_tp btemp = 1;\n      for (int_tp i = dims - 1; i >= 0; --i) {\n        bidx += btemp * (tmp_idx[i] + pad[i]);\n        btemp *= shape_b[i];\n      }\n      bottom_b[bidx] = (backward_b == 1) ? top[index] : 0;\n    }\n  }\n}";  // NOLINT
std::string pooling_float = "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(max_pool_forward,Dtype)(\n    const int_tp nthreads, __global const Dtype* bottom_data, const int_tp num,\n    const int_tp channels, const int_tp height, const int_tp width,\n    const int_tp pooled_height, const int_tp pooled_width, const int_tp kernel_h,\n    const int_tp kernel_w, const int_tp stride_h, const int_tp stride_w, const int_tp pad_h,\n    const int_tp pad_w,\n    __global Dtype* top_data,\n    const int use_mask, __global int_tp* mask, __global Dtype* top_mask) {\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    const int_tp pw = index % pooled_width;\n    const int_tp ph = (index / pooled_width) % pooled_height;\n    const int_tp c = (index / pooled_width / pooled_height) % channels;\n    const int_tp n = index / pooled_width / pooled_height / channels;\n    int_tp hstart = ph * stride_h - pad_h;\n    int_tp wstart = pw * stride_w - pad_w;\n    const int_tp hend = min(hstart + kernel_h, height);\n    const int_tp wend = min(wstart + kernel_w, width);\n    hstart = max(hstart, 0L);\n    wstart = max(wstart, 0L);\n    Dtype maxval = -FLT_MAX;\n    int_tp maxidx = -1;\n    __global const Dtype* bottom_slice = bottom_data\n        + (n * channels + c) * height * width;\n    for (int_tp h = hstart; h < hend; ++h) {\n      for (int_tp w = wstart; w < wend; ++w) {\n        if (bottom_slice[h * width + w] > maxval) {\n          maxidx = h * width + w;\n          maxval = bottom_slice[maxidx];\n        }\n      }\n    }\n    top_data[index] = maxval;\n    if (use_mask == 1) {\n      mask[index] = maxidx;\n    } else {\n      top_mask[index] = maxidx;\n    }\n  }\n}\n\n__kernel void TEMPLATE(ave_pool_forward,Dtype)(\n    const int_tp nthreads, __global const Dtype* const bottom_data, const int_tp num,\n    const int_tp channels, const int_tp height, const int_tp width,\n    const int_tp pooled_height, const int_tp pooled_width, const int_tp kernel_h,\n    const int_tp kernel_w, const int_tp stride_h, const int_tp stride_w, const int_tp pad_h,\n    const int_tp pad_w, __global Dtype* top_data) {\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    {\n      const int_tp pw = index % pooled_width;\n      const int_tp ph = (index / pooled_width) % pooled_height;\n      const int_tp c = (index / pooled_width / pooled_height) % channels;\n      const int_tp n = index / pooled_width / pooled_height / channels;\n      int_tp hstart = ph * stride_h - pad_h;\n      int_tp wstart = pw * stride_w - pad_w;\n      int_tp hend = min(hstart + kernel_h, height + pad_h);\n      int_tp wend = min(wstart + kernel_w, width + pad_w);\n      const int_tp pool_size = (hend - hstart) * (wend - wstart);\n      hstart = max(hstart, 0L);\n      wstart = max(wstart, 0L);\n      hend = min(hend, height);\n      wend = min(wend, width);\n      Dtype aveval = 0;\n      __global const Dtype* bottom_slice = bottom_data\n          + (n * channels + c) * height * width;\n      for (int_tp h = hstart; h < hend; ++h) {\n        for (int_tp w = wstart; w < wend; ++w) {\n          aveval += bottom_slice[h * width + w];\n        }\n      }\n      top_data[index] = aveval / pool_size;\n    }\n  }\n}\n\n__kernel void TEMPLATE(sto_pool_forward_train,Dtype)(\n    const int_tp nthreads, __global const Dtype* bottom_data, const int_tp num,\n    const int_tp channels, const int_tp height, const int_tp width,\n    const int_tp pooled_height, const int_tp pooled_width, const int_tp kernel_h,\n    const int_tp kernel_w, const int_tp stride_h, const int_tp stride_w,\n    __global Dtype* rand_idx,\n    __global Dtype* top_data) {\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    const int_tp pw = index % pooled_width;\n    const int_tp ph = (index / pooled_width) % pooled_height;\n    const int_tp c = (index / pooled_width / pooled_height) % channels;\n    const int_tp n = index / pooled_width / pooled_height / channels;\n    const int_tp hstart = ph * stride_h;\n    const int_tp hend = min(hstart + kernel_h, height);\n    const int_tp wstart = pw * stride_w;\n    const int_tp wend = min(wstart + kernel_w, width);\n    Dtype cumsum = 0.;\n    __global const Dtype* bottom_slice = bottom_data\n        + (n * channels + c) * height * width;\n    // First pass: get sum\n    for (int_tp h = hstart; h < hend; ++h) {\n      for (int_tp w = wstart; w < wend; ++w) {\n        cumsum += bottom_slice[h * width + w];\n      }\n    }\n    const float thres = rand_idx[index] * cumsum;\n    // Second pass: get value, and set index.\n    cumsum = 0;\n    for (int_tp h = hstart; h < hend; ++h) {\n      for (int_tp w = wstart; w < wend; ++w) {\n        cumsum += bottom_slice[h * width + w];\n        if (cumsum >= thres) {\n          rand_idx[index] = ((n * channels + c) * height + h) * width + w;\n          top_data[index] = bottom_slice[h * width + w];\n          h = hend;\n          w = wend;\n        }\n      }\n    }\n  }\n}\n\n__kernel void TEMPLATE(sto_pool_forward_test,Dtype)(\n    const int_tp nthreads, __global const Dtype* const bottom_data, const int_tp num,\n    const int_tp channels, const int_tp height, const int_tp width,\n    const int_tp pooled_height, const int_tp pooled_width, const int_tp kernel_h,\n    const int_tp kernel_w, const int_tp stride_h, const int_tp stride_w,\n    __global Dtype* top_data) {\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    const int_tp pw = index % pooled_width;\n    const int_tp ph = (index / pooled_width) % pooled_height;\n    const int_tp c = (index / pooled_width / pooled_height) % channels;\n    const int_tp n = index / pooled_width / pooled_height / channels;\n    const int_tp hstart = ph * stride_h;\n    const int_tp hend = min(hstart + kernel_h, height);\n    const int_tp wstart = pw * stride_w;\n    const int_tp wend = min(wstart + kernel_w, width);\n    // We set cumsum to be 0 to avoid divide-by-zero problems\n    Dtype cumsum = FLT_MIN;\n    Dtype cumvalues = 0.;\n    __global const Dtype* bottom_slice = bottom_data\n        + (n * channels + c) * height * width;\n    // First pass: get sum\n    for (int_tp h = hstart; h < hend; ++h) {\n      for (int_tp w = wstart; w < wend; ++w) {\n        cumsum += bottom_slice[h * width + w];\n        cumvalues += bottom_slice[h * width + w] * bottom_slice[h * width + w];\n      }\n    }\n    top_data[index] = cumvalues / cumsum;\n  }\n}\n\n__kernel void TEMPLATE(max_pool_backward,Dtype)(const int_tp nthreads,\n                                                __global const Dtype* top_diff,\n                                                const int use_mask,\n                                                __global const int_tp* mask,\n                                                __global const Dtype* top_mask,\n                                                const int_tp num,\n                                                const int_tp channels,\n                                                const int_tp height,\n                                                const int_tp width,\n                                                const int_tp pooled_height,\n                                                const int_tp pooled_width,\n                                                const int_tp kernel_h,\n                                                const int_tp kernel_w,\n                                                const int_tp stride_h,\n                                                const int_tp stride_w,\n                                                const int_tp pad_h,\n                                                const int_tp pad_w,\n                                                __global Dtype* bottom_diff) {\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    // find out the local index\n    // find out the local offset\n    const int_tp w = index % width;\n    const int_tp h = (index / width) % height;\n    const int_tp c = (index / width / height) % channels;\n    const int_tp n = index / width / height / channels;\n    const int_tp phstart =\n        (h + pad_h < kernel_h) ? 0 : (h + pad_h - kernel_h) / stride_h + 1;\n    const int_tp phend = min((h + pad_h) / stride_h + 1, pooled_height);\n    const int_tp pwstart =\n        (w + pad_w < kernel_w) ? 0 : (w + pad_w - kernel_w) / stride_w + 1;\n    const int_tp pwend = min((w + pad_w) / stride_w + 1, pooled_width);\n    Dtype gradient = 0;\n    const int_tp offset = (n * channels + c) * pooled_height * pooled_width;\n    __global const Dtype* top_diff_slice = top_diff + offset;\n    if (use_mask == 1) {\n      __global const int_tp* mask_slice = mask + offset;\n      for (int_tp ph = phstart; ph < phend; ++ph) {\n        for (int_tp pw = pwstart; pw < pwend; ++pw) {\n          if (mask_slice[ph * pooled_width + pw] == h * width + w) {\n            gradient += top_diff_slice[ph * pooled_width + pw];\n          }\n        }\n      }\n    } else {\n      __global const Dtype* top_mask_slice = top_mask + offset;\n      for (int_tp ph = phstart; ph < phend; ++ph) {\n        for (int_tp pw = pwstart; pw < pwend; ++pw) {\n          if (top_mask_slice[ph * pooled_width + pw] == h * width + w) {\n            gradient += top_diff_slice[ph * pooled_width + pw];\n          }\n        }\n      }\n    }\n    bottom_diff[index] = gradient;\n  }\n}\n\n__kernel void TEMPLATE(ave_pool_backward,Dtype)(const int_tp nthreads,\n                                                __global const Dtype* top_diff,\n                                                const int_tp num,\n                                                const int_tp channels,\n                                                const int_tp height,\n                                                const int_tp width,\n                                                const int_tp pooled_height,\n                                                const int_tp pooled_width,\n                                                const int_tp kernel_h,\n                                                const int_tp kernel_w,\n                                                const int_tp stride_h,\n                                                const int_tp stride_w,\n                                                const int_tp pad_h,\n                                                const int_tp pad_w,\n                                                __global Dtype* bottom_diff) {\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    // find out the local index\n    // find out the local offset\n    const int_tp w = index % width + pad_w;\n    const int_tp h = (index / width) % height + pad_h;\n    const int_tp c = (index / width / height) % channels;\n    const int_tp n = index / width / height / channels;\n    const int_tp phstart = (h < kernel_h) ? 0 : (h - kernel_h) / stride_h + 1;\n    const int_tp phend = min(h / stride_h + 1, pooled_height);\n    const int_tp pwstart = (w < kernel_w) ? 0 : (w - kernel_w) / stride_w + 1;\n    const int_tp pwend = min(w / stride_w + 1, pooled_width);\n    Dtype gradient = 0;\n    __global const Dtype* const top_diff_slice = top_diff\n        + (n * channels + c) * pooled_height * pooled_width;\n    for (int_tp ph = phstart; ph < phend; ++ph) {\n      for (int_tp pw = pwstart; pw < pwend; ++pw) {\n        // figure out the pooling size\n        int_tp hstart = ph * stride_h - pad_h;\n        int_tp wstart = pw * stride_w - pad_w;\n        int_tp hend = min(hstart + kernel_h, height + pad_h);\n        int_tp wend = min(wstart + kernel_w, width + pad_w);\n        int_tp pool_size = (hend - hstart) * (wend - wstart);\n        gradient += top_diff_slice[ph * pooled_width + pw] / pool_size;\n      }\n    }\n    bottom_diff[index] = gradient;\n  }\n}\n\n__kernel void TEMPLATE(sto_pool_backward,Dtype)(\n    const int_tp nthreads, __global const Dtype* rand_idx,\n    __global const Dtype* const top_diff, const int_tp num, const int_tp channels,\n    const int_tp height, const int_tp width, const int_tp pooled_height,\n    const int_tp pooled_width, const int_tp kernel_h, const int_tp kernel_w,\n    const int_tp stride_h, const int_tp stride_w, __global Dtype* bottom_diff) {\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    // find out the local index\n    // find out the local offset\n    const int_tp w = index % width;\n    const int_tp h = (index / width) % height;\n    const int_tp c = (index / width / height) % channels;\n    const int_tp n = index / width / height / channels;\n    const int_tp phstart = (h < kernel_h) ? 0 : (h - kernel_h) / stride_h + 1;\n    const int_tp phend = min(h / stride_h + 1, pooled_height);\n    const int_tp pwstart = (w < kernel_w) ? 0 : (w - kernel_w) / stride_w + 1;\n    const int_tp pwend = min(w / stride_w + 1, pooled_width);\n    Dtype gradient = 0;\n    __global const Dtype* rand_idx_slice = rand_idx\n        + (n * channels + c) * pooled_height * pooled_width;\n    __global const Dtype* top_diff_slice = top_diff\n        + (n * channels + c) * pooled_height * pooled_width;\n    for (int_tp ph = phstart; ph < phend; ++ph) {\n      for (int_tp pw = pwstart; pw < pwend; ++pw) {\n        gradient += top_diff_slice[ph * pooled_width + pw]\n            * (index == (int_tp) (rand_idx_slice[ph * pooled_width + pw]));\n      }\n    }\n    bottom_diff[index] = gradient;\n  }\n}";  // NOLINT
std::string pooling_nd_float = "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(max_pool_forward_nd, Dtype)(const int_tp n,\n                                                   const int_tp num_axes,\n                                                   __global const Dtype* bottom_data,\n                                                   const int_tp channels,\n                                                   __global const int_tp* size,\n                                                   __global const int_tp* pooled_size,\n                                                   __global const int_tp* kernel_size,\n                                                   __global const int_tp* ext_kernel_size,\n                                                   __global const int_tp* stride,\n                                                   __global const int_tp* kstride,\n                                                   __global const int_tp* pad,\n                                                   __global Dtype* top_data,\n                                                   const int use_mask,\n                                                   __global int_tp* mask, __global Dtype* top_mask) {\n  int_tp d_idx[6];\n  int_tp d_start[6];\n  int_tp d_end[6];\n  int_tp d_iter[6];\n  int_tp i;\n\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    int_tp offset = 1;\n    int_tp num = index;\n\n    bool do_continue = false;\n\n    for (i = num_axes - 1; i >= 0; --i) {\n      d_idx[i] = num % pooled_size[i];\n      d_start[i] = d_idx[i] * stride[i] - pad[i];\n      d_end[i] = min(d_start[i] + ext_kernel_size[i], size[i]);\n      d_start[i] = max(d_start[i], 0L);\n      num /= pooled_size[i];\n      offset *= size[i];\n      d_iter[i] = d_start[i];\n\n      if (d_start[i] >= d_end[i]) {\n        top_data[index] = -FLT_MAX;\n        if (use_mask) {\n          mask[index] = -1;\n        } else {\n          top_mask[index] = -1;\n        }\n        do_continue = true;\n      }\n    }\n\n    if(do_continue) {\n      continue;\n    }\n\n    int_tp chan = num % channels;\n    num /= channels;\n    offset *= (num * channels + chan);\n\n    Dtype maxval = -FLT_MAX;\n    int_tp maxidx = -1;\n    int_tp final_offset = 0;\n\n    bool incremented;\n    do {\n      final_offset = offset;\n      int_tp size_prod = 1;\n      for (i = num_axes - 1; i >= 0; --i) {\n        final_offset += d_iter[i] * size_prod;\n        size_prod *= size[i];\n      }\n\n      if (bottom_data[final_offset] > maxval) {\n        maxidx = final_offset;\n        maxval = bottom_data[maxidx];\n      }\n\n      incremented = false;\n      for (i = num_axes - 1; i >= 0; --i) {\n        if (d_iter[i] >= d_end[i] - kstride[i]) {\n          d_iter[i] = d_start[i];\n        } else {\n          d_iter[i] += kstride[i];\n          incremented = true;\n          break;\n        }\n      }\n    } while (incremented);\n\n    top_data[index] = maxval;\n    if (use_mask == 1) {\n      mask[index] = maxidx;\n    } else {\n      top_mask[index] = maxidx;\n    }\n  }\n}\n\n\n__kernel void TEMPLATE(max_pool_backward_nd, Dtype)(const int_tp n,\n                                                    const int_tp num_axes,\n                                                    __global const Dtype* top_diff,\n                                                    const int use_mask,\n                                                    __global const int_tp* mask,\n                                                    __global const Dtype* top_mask,\n                                                    const int_tp channels,\n                                                    __global const int_tp* size,\n                                                    __global const int_tp* pooled_size,\n                                                    __global const int_tp* kernel_size,\n                                                    __global const int_tp* ext_kernel_size,\n                                                    __global const int_tp* stride,\n                                                    __global const int_tp* kstride,\n                                                    __global const int_tp* pad,\n                                                    __global Dtype* bottom_diff) {\n  int_tp d_idx[6];\n  int_tp d_start[6];\n  int_tp d_end[6];\n  int_tp d_iter[6];\n  int_tp i;\n\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    // find out the local index\n    // find out the local offset\n    int_tp offset = 1;\n    int_tp num = index;\n    for (i = num_axes - 1; i >= 0; --i) {\n      d_idx[i] = num % size[i];\n      if (kstride[i] > 1) {\n        d_start[i] =\n            (d_idx[i] < ext_kernel_size[i]) ?\n                d_idx[i] % kstride[i] : (d_idx[i] - ext_kernel_size[i]) + 1;\n        d_end[i] =\n            (d_idx[i] >= pooled_size[i]) ?\n                (pooled_size[i] - 1)\n                    - (pooled_size[i] - 1 - d_start[i]) % kstride[i] :\n                d_idx[i];\n      } else {\n        d_start[i] =\n            (d_idx[i] + pad[i] < kernel_size[i]) ?\n                0 : (d_idx[i] + pad[i] - kernel_size[i]) / stride[i] + 1;\n        d_end[i] = min((int_tpc) ((d_idx[i] + pad[i]) / stride[i] + 1),\n                       (int_tpc) (pooled_size[i]));\n      }\n      num /= size[i];\n      offset *= pooled_size[i];\n      d_iter[i] = d_start[i];\n\n      if (d_start[i] > d_end[i]) {\n        bottom_diff[index] = 0;\n        return;\n      }\n    }\n    int_tp chan = num % channels;\n    num /= channels;\n    offset *= (num * channels + chan);\n\n    Dtype gradient = 0;\n    int_tp final_offset = 0;\n    int_tp im_offset = 0;\n\n    bool incremented;\n    do {\n      final_offset = offset;\n      im_offset = 0;\n      int_tp size_prod = 1;\n      int_tp pooled_size_prod = 1;\n      for (i = num_axes - 1; i >= 0; --i) {\n        final_offset += d_iter[i] * pooled_size_prod;\n        im_offset += d_idx[i] * size_prod;\n        size_prod *= size[i];\n        pooled_size_prod *= pooled_size[i];\n      }\n\n      if (use_mask) {\n        if (mask[final_offset] == im_offset) {\n          gradient += top_diff[final_offset];\n        }\n      } else {\n        if (top_mask[final_offset] == im_offset) {\n          gradient += top_diff[final_offset];\n        }\n      }\n\n      incremented = false;\n      for (i = num_axes - 1; i >= 0; --i) {\n        if (d_iter[i] > d_end[i] - kstride[i]) {\n          d_iter[i] = d_start[i];\n        } else {\n          d_iter[i] += kstride[i];\n          incremented = true;\n          break;\n        }\n      }\n    } while (incremented);\n    bottom_diff[index] = gradient;\n  }\n}";  // NOLINT
std::string pooling_sk_float = "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(max_pool_forward_sk,Dtype)(const int_tp nthreads,\n__global Dtype* bottom_data,\n                                                  const int_tp num,\n                                                  const int_tp channels,\n                                                  const int_tp height,\n                                                  const int_tp width,\n                                                  const int_tp pooled_height,\n                                                  const int_tp pooled_width,\n                                                  const int_tp kernel_h,\n                                                  const int_tp kernel_w,\n                                                  const int_tp ext_kernel_h,\n                                                  const int_tp ext_kernel_w,\n                                                  const int_tp stride_h,\n                                                  const int_tp stride_w,\n                                                  const int_tp kstride_h,\n                                                  const int_tp kstride_w,\n                                                  const int_tp pad_h,\n                                                  const int_tp pad_w,\n                                                  __global Dtype* top_data,\n                                                  const int use_mask,\n                                                  __global int_tp* mask,\n                                                  __global Dtype* top_mask) {\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    int_tp pw = index % pooled_width;\n    int_tp ph = (index / pooled_width) % pooled_height;\n    int_tp c = (index / pooled_width / pooled_height) % channels;\n    int_tp n = index / pooled_width / pooled_height / channels;\n    int_tp hstart = ph * stride_h - pad_h;\n    int_tp wstart = pw * stride_w - pad_w;\n    int_tp hend = min(hstart + ext_kernel_h, height);\n    int_tp wend = min(wstart + ext_kernel_w, width);\n    hstart = max(hstart, (int_tp) 0);\n    wstart = max(wstart, (int_tp) 0);\n    Dtype maxval = -FLT_MAX;\n    int_tp maxidx = -1;\n    __global Dtype* bottom_data_ptr = bottom_data\n        + (n * channels + c) * height * width;\n    for (int_tp h = hstart; h < hend; h += kstride_h) {\n      for (int_tp w = wstart; w < wend; w += kstride_w) {\n        if (bottom_data_ptr[h * width + w] > maxval) {\n          maxidx = h * width + w;\n          maxval = bottom_data_ptr[maxidx];\n        }\n      }\n    }\n    top_data[index] = maxval;\n    if (use_mask == 1) {\n      mask[index] = maxidx;\n    } else {\n      top_mask[index] = maxidx;\n    }\n  }\n}\n\n__kernel void TEMPLATE(max_pool_backward_sk,Dtype)(\n    const int_tp nthreads, __global const Dtype* top_diff, const int use_mask,\n    __global const int_tp* mask, __global const Dtype* top_mask, const int_tp num,\n    const int_tp channels, const int_tp height, const int_tp width,\n    const int_tp pooled_height, const int_tp pooled_width, const int_tp kernel_h,\n    const int_tp kernel_w, const int_tp ext_kernel_h, const int_tp ext_kernel_w,\n    const int_tp stride_h, const int_tp stride_w, const int_tp kstride_h,\n    const int_tp kstride_w, const int_tp pad_h, const int_tp pad_w,\n    __global Dtype* bottom_diff) {\n\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n\n    __global const int_tp* mask_ptr = mask;\n    __global const Dtype* top_diff_ptr = top_diff;\n\n// find out the local index\n// find out the local offset\n    int_tp w = index % width;\n    int_tp h = (index / width) % height;\n    int_tp c = (index / width / height) % channels;\n    int_tp n = index / width / height / channels;\n\n    int_tp pooled_height_1 = pooled_height - 1;\n    int_tp pooled_width_1 = pooled_width - 1;\n    int_tp phstart = (h < ext_kernel_h) ? h % kstride_h : (h - ext_kernel_h) + 1;\n    int_tp phend =\n        (h >= pooled_height) ?\n            pooled_height_1 - (pooled_height_1 - phstart) % kstride_h : h;\n    int_tp pwstart = (w < ext_kernel_w) ? w % kstride_w : (w - ext_kernel_w) + 1;\n    int_tp pwend =\n        (w >= pooled_width) ?\n            pooled_width_1 - (pooled_width_1 - pwstart) % kstride_w : w;\n\n    Dtype gradient = 0;\n    int_tp offset = (n * channels + c) * pooled_height * pooled_width;\n    top_diff_ptr += offset;\n    if (use_mask == 1) {\n      mask_ptr += offset;\n      for (int_tp ph = phstart; ph <= phend; ph += kstride_h) {\n        for (int_tp pw = pwstart; pw <= pwend; pw += kstride_w) {\n          if (mask_ptr[ph * pooled_width + pw] == h * width + w) {\n            gradient += top_diff_ptr[ph * pooled_width + pw];\n          }\n        }\n      }\n    } else {\n      for (int_tp ph = phstart; ph <= phend; ph += kstride_h) {\n        for (int_tp pw = pwstart; pw <= pwend; pw += kstride_w) {\n          if (top_mask[ph * pooled_width + pw] == h * width + w) {\n            gradient += top_diff_ptr[ph * pooled_width + pw];\n          }\n        }\n      }\n    }\n    bottom_diff[index] = gradient;\n  }\n}\n\n__kernel void TEMPLATE(ave_pool_forward_sk,Dtype)(\n    const int_tp nthreads, __global const Dtype* bottom_data, const int_tp num,\n    const int_tp channels, const int_tp height, const int_tp width,\n    const int_tp pooled_height, const int_tp pooled_width, const int_tp kernel_h,\n    const int_tp kernel_w, const int_tp ext_kernel_h, const int_tp ext_kernel_w,\n    const int_tp stride_h, const int_tp stride_w, const int_tp kstride_h,\n    const int_tp kstride_w, const int_tp pad_h, const int_tp pad_w,\n    __global Dtype* top_data) {\n\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n\n    int_tp pw = index % pooled_width;\n    int_tp ph = (index / pooled_width) % pooled_height;\n    int_tp c = (index / pooled_width / pooled_height) % channels;\n    int_tp n = index / pooled_width / pooled_height / channels;\n    int_tp hstart = ph * stride_h - pad_h;\n    int_tp wstart = pw * stride_w - pad_w;\n    int_tp hend = min(hstart + ext_kernel_h, height + pad_h);\n    int_tp wend = min(wstart + ext_kernel_w, width + pad_w);\n    hstart = max(hstart, 0L);\n    wstart = max(wstart, 0L);\n    hend = min(hend, height);\n    wend = min(wend, width);\n    Dtype aveval = 0;\n    __global const Dtype* bottom_data_ptr = bottom_data;\n    bottom_data_ptr += (n * channels + c) * height * width;\n    int_tp pool_size = 0;\n    for (int_tp h = hstart; h < hend; ++h) {\n      for (int_tp w = wstart; w < wend; ++w) {\n        aveval += bottom_data_ptr[h * width + w];\n        ++pool_size;\n      }\n    }\n    top_data[index] = aveval / pool_size;\n  }\n}\n\n__kernel void TEMPLATE(sto_pool_forward_train_sk,Dtype)(\n    const int_tp nthreads, __global const Dtype* bottom_data, const int_tp num,\n    const int_tp channels, const int_tp height, const int_tp width,\n    const int_tp pooled_height, const int_tp pooled_width, const int_tp kernel_h,\n    const int_tp kernel_w, const int_tp ext_kernel_h, const int_tp ext_kernel_w,\n    const int_tp stride_h, const int_tp stride_w, const int_tp kstride_h,\n    const int_tp kstride_w, __global Dtype* rand_idx,\n    __global Dtype* top_data) {\n\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    int_tp pw = index % pooled_width;\n    int_tp ph = (index / pooled_width) % pooled_height;\n    int_tp c = (index / pooled_width / pooled_height) % channels;\n    int_tp n = index / pooled_width / pooled_height / channels;\n    int_tp hstart = ph * stride_h;\n    int_tp hend = min(hstart + ext_kernel_h, height);\n    int_tp wstart = pw * stride_w;\n    int_tp wend = min(wstart + ext_kernel_w, width);\n    Dtype cumsum = 0.;\n    __global const Dtype* bottom_data_ptr = bottom_data;\n    bottom_data_ptr += (n * channels + c) * height * width;\n    // First pass: get sum\n    for (int_tp h = hstart; h < hend; h += kstride_h) {\n      for (int_tp w = wstart; w < wend; w += kstride_w) {\n        cumsum += bottom_data_ptr[h * width + w];\n      }\n    }\n    float thres = rand_idx[index] * cumsum;\n    // Second pass: get value, and set index.\n    cumsum = 0;\n    for (int_tp h = hstart; h < hend; h += kstride_h) {\n      for (int_tp w = wstart; w < wend; w += kstride_w) {\n        cumsum += bottom_data_ptr[h * width + w];\n        if (cumsum >= thres) {\n          rand_idx[index] = ((n * channels + c) * height + h) * width + w;\n          top_data[index] = bottom_data_ptr[h * width + w];\n          h = hend;\n          w = wend;\n        }\n      }\n    }\n  }\n}\n\n__kernel void TEMPLATE(sto_pool_forward_test_sk,Dtype)(\n    const int_tp nthreads, __global const Dtype* bottom_data, const int_tp num,\n    const int_tp channels, const int_tp height, const int_tp width,\n    const int_tp pooled_height, const int_tp pooled_width, const int_tp kernel_h,\n    const int_tp kernel_w, const int_tp ext_kernel_h, const int_tp ext_kernel_w,\n    const int_tp stride_h, const int_tp stride_w, const int_tp kstride_h,\n    const int_tp kstride_w,\n    __global Dtype* top_data) {\n\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    int_tp pw = index % pooled_width;\n    int_tp ph = (index / pooled_width) % pooled_height;\n    int_tp c = (index / pooled_width / pooled_height) % channels;\n    int_tp n = index / pooled_width / pooled_height / channels;\n    int_tp hstart = ph * stride_h;\n    int_tp hend = min(hstart + ext_kernel_h, height);\n    int_tp wstart = pw * stride_w;\n    int_tp wend = min(wstart + ext_kernel_w, width);\n    // We set cumsum to be 0 to avoid divide-by-zero problems\n    Dtype cumsum = FLT_MIN;\n    Dtype cumvalues = 0.;\n    __global const Dtype* bottom_data_ptr = bottom_data;\n    bottom_data_ptr += (n * channels + c) * height * width;\n    // First pass: get sum\n    for (int_tp h = hstart; h < hend; h += kstride_h) {\n      for (int_tp w = wstart; w < wend; w += kstride_w) {\n        cumsum += bottom_data_ptr[h * width + w];\n        cumvalues += bottom_data_ptr[h * width + w]\n            * bottom_data_ptr[h * width + w];\n      }\n    }\n    top_data[index] = cumvalues / cumsum;\n  }\n\n}";  // NOLINT
std::string slice_float = "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(slice,Dtype)(const int_tp nthreads,\n                                    __global const Dtype* in_data,\n                                    const int forward, const int_tp num_slices,\n                                    const int_tp slice_size,\n                                    const int_tp bottom_slice_axis,\n                                    const int_tp top_slice_axis,\n                                    const int_tp offset_slice_axis,\n                                    __global Dtype* out_data) {\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    const int_tp total_slice_size = slice_size * top_slice_axis;\n    const int_tp slice_num = index / total_slice_size;\n    const int_tp slice_index = index % total_slice_size;\n    const int_tp bottom_index = slice_index\n        + (slice_num * bottom_slice_axis + offset_slice_axis) * slice_size;\n    if (forward == 1) {\n      out_data[index] = in_data[bottom_index];\n    } else {\n      out_data[bottom_index] = in_data[index];\n    }\n  }\n}";  // NOLINT
std::string softmax_loss_float = "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(softmax_loss_forward,Dtype)(\n    int_tp n, __global const Dtype* prob_data, __global const Dtype* label,\n    __global Dtype* loss,\n    const int_tp num, const int_tp dim, const int_tp spatial_dim,\n    const int has_ignore_label_, const int_tp ignore_label_,\n    __global Dtype* counts) {\n\n  for (int_tp index = get_global_id(0); index < n;\n      index += get_global_size(0)) {\n    const int_tp n = index / spatial_dim;\n    const int_tp s = index % spatial_dim;\n    const int_tp label_value = (int_tp) (label[n * spatial_dim + s]);\n    if (has_ignore_label_ == 1 && label_value == ignore_label_) {\n      loss[index] = 0;\n      counts[index] = 0;\n    } else {\n      loss[index] = -log(\n          max((Dtype) (prob_data[n * dim + label_value * spatial_dim + s]),\n              (Dtype) FLT_MIN));\n      counts[index] = 1;\n    }\n  }\n}\n\n__kernel void TEMPLATE(softmax_loss_backward,Dtype)(const int_tp nthreads,\n                                                    __global const Dtype* top,\n                                                    __global const Dtype* label,\n                                                    __global Dtype* bottom_diff,\n                                                    const int_tp num,\n                                                    const int_tp dim,\n                                                    const int_tp spatial_dim,\n                                                    const int has_ignore_label_,\n                                                    const int_tp ignore_label_,\n                                                    __global Dtype* counts) {\n\n  const int_tp channels = dim / spatial_dim;\n\n  for (int_tp index = get_global_id(0); index < nthreads; index +=\n      get_global_size(0)) {\n\n    const int_tp n = index / spatial_dim;\n    const int_tp s = index % spatial_dim;\n    const int_tp label_value = (int_tp) (label[n * spatial_dim + s]);\n\n    if (has_ignore_label_ == 1 && label_value == ignore_label_) {\n      for (int_tp c = 0; c < channels; ++c) {\n        bottom_diff[n * dim + c * spatial_dim + s] = 0;\n      }\n      counts[index] = 0;\n    } else {\n      bottom_diff[n * dim + label_value * spatial_dim + s] -= 1;\n      counts[index] = 1;\n    }\n  }\n}";  // NOLINT
std::string tile_float = "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n\n__kernel void TEMPLATE(tile,Dtype)(const int_tp nthreads, __global const Dtype* bottom_data,\n                                   const int_tp tile_size, const int_tp num_tiles,\n                                   const int_tp bottom_tile_axis,\n                                   __global Dtype* top_data) {\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    const int_tp d = index % tile_size;\n    const int_tp b = (index / tile_size / num_tiles) % bottom_tile_axis;\n    const int_tp n = index / tile_size / num_tiles / bottom_tile_axis;\n    const int_tp bottom_index = (n * bottom_tile_axis + b) * tile_size + d;\n    top_data[index] = bottom_data[bottom_index];\n  }\n}\n\n\n__kernel void TEMPLATE(tile_backward,Dtype)(const int_tp nthreads,\n                                            __global const Dtype* top_diff,\n                                            const int_tp tile_size,\n                                            const int_tp num_tiles,\n                                            const int_tp bottom_tile_axis,\n                                            __global Dtype* bottom_diff) {\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    const int_tp d = index % tile_size;\n    const int_tp b = (index / tile_size) % bottom_tile_axis;\n    const int_tp n = index / tile_size / bottom_tile_axis;\n    bottom_diff[index] = 0;\n    int_tp top_index = (n * num_tiles * bottom_tile_axis + b) * tile_size + d;\n    for (int_tp t = 0; t < num_tiles; ++t) {\n      bottom_diff[index] += top_diff[top_index];\n      top_index += bottom_tile_axis * tile_size;\n    }\n  }\n}";  // NOLINT
std::string activation_double = "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(relu_forward,Dtype)(const int_tp n,\n                                           __global const Dtype* in,\n                                           __global Dtype* out,\n                                           Dtype negative_slope) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    out[index] = in[index] > 0 ? in[index] : in[index] * negative_slope;\n  }\n}\n\n__kernel void TEMPLATE(relu_backward,Dtype)(const int_tp n,\n                                            __global const Dtype* in_diff,\n                                            __global const Dtype* in_data,\n                                            __global Dtype* out_diff,\n                                            Dtype negative_slope) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    out_diff[index] = in_diff[index]\n        * ((in_data[index] > 0) + (in_data[index] <= 0) * negative_slope);\n  }\n}\n\n__kernel void TEMPLATE(tanh_forward,Dtype)(const int_tp n,\n                                           __global const Dtype* in,\n                                           __global Dtype* out) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    out[index] = tanh(in[index]);\n  }\n}\n\n__kernel void TEMPLATE(tanh_backward,Dtype)(const int_tp n,\n                                            __global const Dtype* in_diff,\n                                            __global const Dtype* out_data,\n                                            __global Dtype* out_diff) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    Dtype tanhx = out_data[index];\n    out_diff[index] = in_diff[index] * (1 - tanhx * tanhx);\n  }\n}\n\n__kernel void TEMPLATE(sigmoid_forward,Dtype)(const int_tp n,\n                                              __global const Dtype* in,\n                                              __global Dtype* out) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    out[index] = 1. / (1. + exp(-in[index]));\n  }\n}\n\n__kernel void TEMPLATE(sigmoid_backward,Dtype)(const int_tp n,\n                                               __global const Dtype* in_diff,\n                                               __global const Dtype* out_data,\n                                               __global Dtype* out_diff) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    const Dtype sigmoid_x = out_data[index];\n    out_diff[index] = in_diff[index] * sigmoid_x * (1 - sigmoid_x);\n  }\n}\n\n__kernel void TEMPLATE(threshold,Dtype)(const int_tp n, const Dtype threshold,\n                                        __global const Dtype* in,\n                                        __global Dtype* out) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    out[index] = in[index] > threshold ? 1 : 0;\n  }\n}\n\n__kernel void TEMPLATE(prelu_forward,Dtype)(const int_tp n, const int_tp channels,\n                                            const int_tp dim,\n                                            __global const Dtype* in,\n                                            __global Dtype* out,\n                                            __global const Dtype* slope_data,\n                                            const int_tp div_factor) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    int_tp c = (index / dim) % channels / div_factor;\n    out[index] = in[index] > 0 ? in[index] : in[index] * slope_data[c];\n  }\n}\n\n__kernel void TEMPLATE(prelu_backward,Dtype)(const int_tp n, const int_tp channels,\n                                             const int_tp dim,\n                                             __global const Dtype* in_diff,\n                                             __global const Dtype* in_data,\n                                             __global Dtype* out_diff,\n                                             __global const Dtype* slope_data,\n                                             const int_tp div_factor) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    int_tp c = (index / dim) % channels / div_factor;\n    out_diff[index] = in_diff[index]\n        * ((in_data[index] > 0) + (in_data[index] <= 0) * slope_data[c]);\n  }\n}\n\n__kernel void TEMPLATE(prelu_param_backward,Dtype)(const int_tp n, const int_tp rows,\n                                                   const int_tp rowPitch,\n                                                   __global const Dtype* in_diff,\n                                                   __global const Dtype* in_data,\n                                                   __global Dtype* out_diff) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    out_diff[index] = in_diff[index] * in_data[index] * (in_data[index] <= 0);\n    for (int k = 1; k < rows; k++) {\n      out_diff[index] += in_diff[index + k * rowPitch]\n          * in_data[index + k * rowPitch]\n          * (in_data[index + k * rowPitch] <= 0);\n    }\n  }\n}";  // NOLINT
std::string auxiliary_double = "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(gpu_set,Dtype)(const int_tp n, const Dtype alpha, __global Dtype* y) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    y[index] = alpha;\n  }\n}";  // NOLINT
std::string batch_reindex_double = "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(br_forward,Dtype)(const int_tp count, const int_tp inner_dim,\n                                         __global const Dtype* in,\n                                         __global const Dtype* permut,\n                                         __global Dtype* out) {\n  for (int_tp index = get_global_id(0); index < count;\n      index += get_global_size(0)) {\n    int_tp n = index / (inner_dim);\n    int_tp in_n = (int_tp) (permut[n]);\n    out[index] = in[in_n * (inner_dim) + index % (inner_dim)];\n  }\n}\n\n__kernel void TEMPLATE(br_backward,Dtype)(const int_tp count, const int_tp inner_dim,\n                                          __global const Dtype* in,\n                                          __global const Dtype* top_indexes,\n                                          __global const Dtype* begins,\n                                          __global const Dtype* counts,\n                                          __global Dtype* out) {\n  for (int_tp index = get_global_id(0); index < count;\n      index += get_global_size(0)) {\n    int_tp n = index / (inner_dim);\n    out[index] = 0;\n    int_tp lower = (int_tp) (begins[n]);\n    int_tp upper = lower + (int_tp) (counts[n]);\n    for (int_tp i = lower; i < upper; ++i) {\n      int_tp in_n = (int_tp) (top_indexes[i]);\n      out[index] += in[in_n * (inner_dim) + index % (inner_dim)];\n    }\n  }\n}";  // NOLINT
std::string bnll_double = "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(bnll_forward,Dtype)(const int_tp n,\n                                           __global const Dtype* in,\n                                           __global Dtype* out) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    out[index] =\n        in[index] > 0 ?\n            in[index] + log(1. + exp(-in[index])) : log(1. + exp(in[index]));\n  }\n}\n\n__kernel void TEMPLATE(bnll_backward,Dtype)(const int_tp n,\n                                            __global const Dtype* in_diff,\n                                            __global const Dtype* in_data,\n                                            __global Dtype* out_diff) {\n  Dtype kBNLL_THRESHOLD = 50.;\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    Dtype expval = exp(min(in_data[index], kBNLL_THRESHOLD));\n    out_diff[index] = in_diff[index] * expval / (expval + 1.);\n  }\n}";  // NOLINT
std::string channel_double = "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(kernel_channel_max,Dtype)(const int_tp num, const int_tp channels,\n                                   const int_tp spatial_dim,\n                                   __global const Dtype* data,\n                                   __global Dtype* out) {\n  for (int_tp index = get_global_id(0); index < num * spatial_dim; index +=\n      get_global_size(0)) {\n    int_tp n = index / spatial_dim;\n    int_tp s = index % spatial_dim;\n    float maxval = -FLT_MAX;\n    for (int_tp c = 0; c < channels; ++c) {\n      maxval = max((Dtype)(data[(n * channels + c) * spatial_dim + s]), (Dtype)maxval);\n    }\n    out[index] = maxval;\n  }\n}\n\n__kernel void TEMPLATE(kernel_channel_subtract,Dtype)(const int_tp count, const int_tp num,\n                                        const int_tp channels,\n                                        const int_tp spatial_dim,\n                                        __global const Dtype* channel_max,\n                                        __global Dtype* data) {\n  for (int_tp index = get_global_id(0); index < count;\n      index += get_global_size(0)) {\n    int_tp n = index / channels / spatial_dim;\n    int_tp s = index % spatial_dim;\n    data[index] -= channel_max[n * spatial_dim + s];\n  }\n}\n\n__kernel void TEMPLATE(kernel_exp,Dtype)(const int_tp count, __global const Dtype* data,\n                           __global Dtype* out) {\n  for (int_tp index = get_global_id(0); index < count;\n      index += get_global_size(0)) {\n    out[index] = exp(data[index]);\n  }\n}\n\n__kernel void TEMPLATE(kernel_channel_sum,Dtype)(const int_tp num, const int_tp channels,\n                                   const int_tp spatial_dim,\n                                   __global const Dtype* data,\n                                   __global Dtype* channel_sum) {\n  for (int_tp index = get_global_id(0); index < num * spatial_dim; index +=\n      get_global_size(0)) {\n    int_tp n = index / spatial_dim;\n    int_tp s = index % spatial_dim;\n    Dtype sum = 0;\n    for (int_tp c = 0; c < channels; ++c) {\n      sum += data[(n * channels + c) * spatial_dim + s];\n    }\n    channel_sum[index] = sum;\n  }\n}\n\n__kernel void TEMPLATE(kernel_channel_div,Dtype)(const int_tp count, const int_tp num,\n                                   const int_tp channels, const int_tp spatial_dim,\n                                   __global const Dtype* channel_sum,\n                                   __global Dtype* data) {\n  for (int_tp index = get_global_id(0); index < count;\n      index += get_global_size(0)) {\n    int_tp n = index / channels / spatial_dim;\n    int_tp s = index % spatial_dim;\n    data[index] /= channel_sum[n * spatial_dim + s];\n  }\n}\n\n__kernel void TEMPLATE(kernel_channel_dot,Dtype)(const int_tp num, const int_tp channels,\n                                   const int_tp spatial_dim,\n                                   __global const Dtype* data_1,\n                                   __global const Dtype* data_2,\n                                   __global Dtype* channel_dot) {\n  for (int_tp index = get_global_id(0); index < num * spatial_dim; index +=\n      get_global_size(0)) {\n    int_tp n = index / spatial_dim;\n    int_tp s = index % spatial_dim;\n    Dtype dot = 0;\n    for (int_tp c = 0; c < channels; ++c) {\n      dot += (data_1[(n * channels + c) * spatial_dim + s]\n          * data_2[(n * channels + c) * spatial_dim + s]);\n    }\n    channel_dot[index] = dot;\n  }\n}";  // NOLINT
std::string concat_double = "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(concat,Dtype)(const int_tp nthreads, __global const Dtype* in_data,\n                                     const int forward, const int_tp num_concats,\n                                     const int_tp concat_size,\n                                     const int_tp top_concat_axis,\n                                     const int_tp bottom_concat_axis,\n                                     const int_tp offset_concat_axis,\n                                     __global Dtype* out_data) {\n\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    const int_tp total_concat_size = concat_size * bottom_concat_axis;\n    const int_tp concat_num = index / total_concat_size;\n    const int_tp concat_index = index % total_concat_size;\n    const int_tp top_index = concat_index\n        + (concat_num * top_concat_axis + offset_concat_axis) * concat_size;\n    if (forward == 1) {\n      out_data[top_index] = in_data[index];\n    } else {\n      out_data[index] = in_data[top_index];\n    }\n  }\n}";  // NOLINT
std::string contrastive_loss_double = "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(cll_backward,Dtype)(const int_tp count, const int_tp channels,\n                            const Dtype margin, const int legacy_version,\n                            const Dtype alpha, __global const Dtype* y,\n                            __global const Dtype* diff, __global const Dtype* dist_sq,\n                            __global Dtype *bottom_diff) {\n  for (int_tp i = get_global_id(0); i < count;\n      i += get_global_size(0)) {\n    int_tp n = i / channels;  // the num index, to access y and dist_sq\n    if ((int_tp)(y[n])) {  // similar pairs\n      bottom_diff[i] = alpha * diff[i];\n    } else {  // dissimilar pairs\n      Dtype mdist = 0.0;\n      Dtype beta = 0.0;\n      if (legacy_version == 1) {\n        mdist = (margin - dist_sq[n]);\n        beta = -alpha;\n      } else {\n        Dtype dist = sqrt(dist_sq[n]);\n        mdist = (margin - dist);\n        beta = -alpha * mdist / (dist + 1e-4) * diff[i];\n      }\n      if (mdist > 0.0) {\n        bottom_diff[i] = beta;\n      } else {\n        bottom_diff[i] = 0;\n      }\n    }\n  }\n}";  // NOLINT
std::string dropout_double = "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(dropout_forward,Dtype)(const int_tp n,\n                                              __global const Dtype* in,\n                                              __global const uint_tp* mask,\n                                              const uint_tp threshold,\n                                              const Dtype scale,\n                                              __global Dtype* out) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    out[index] = in[index] * ((Dtype)(mask[index] > threshold)) * scale;\n  }\n}\n\n__kernel void TEMPLATE(dropout_backward,Dtype)(\n    const int_tp n, __global const Dtype* in_diff,\n    __global const uint_tp* mask, const uint_tp threshold,\n    const Dtype scale,\n    __global Dtype* out_diff) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    out_diff[index] = in_diff[index] * scale * ((Dtype)(mask[index] > threshold));\n  }\n}";  // NOLINT
std::string eltwise_double = "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(eltwise_max_forward,Dtype)(\n    const int_tp nthreads, __global const Dtype* bottom_data_a,\n    __global const Dtype* bottom_data_b, const int_tp blob_idx,\n    __global Dtype* top_data,\n    __global int_tp* mask) {\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    Dtype maxval = -FLT_MAX;\n    int_tp maxidx = -1;\n    if (bottom_data_a[index] > bottom_data_b[index]) {\n      // only update for very first bottom_data blob (blob_idx == 0)\n      if (blob_idx == 0) {\n        maxval = bottom_data_a[index];\n        top_data[index] = maxval;\n        maxidx = blob_idx;\n        mask[index] = maxidx;\n      }\n    } else {\n      maxval = bottom_data_b[index];\n      top_data[index] = maxval;\n      maxidx = blob_idx + 1;\n      mask[index] = maxidx;\n    }\n  }\n}\n\n__kernel void TEMPLATE(eltwise_max_backward,Dtype)(const int_tp nthreads,\n                                                   __global const Dtype* top_diff,\n                                                   const int_tp blob_idx,\n                                                   __global const int_tp* mask,\n                                                   __global Dtype* bottom_diff) {\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    Dtype gradient = 0;\n    if (mask[index] == blob_idx) {\n      gradient += top_diff[index];\n    }\n    bottom_diff[index] = gradient;\n  }\n}";  // NOLINT
std::string embed_double = "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(embed_forward,Dtype)(const int_tp nthreads,\n                                            __global const Dtype* bottom_data,\n                                            __global const Dtype* weight,\n                                            const int_tp M, const int_tp N,\n                                            const int_tp K,\n                                            __global Dtype* top_data) {\n  for (int_tp top_index = get_global_id(0); top_index < nthreads;\n      top_index += get_global_size(0)) {\n      const int_tp n = top_index / N;\n      const int_tp d = top_index % N;\n      const int_tp index = (int_tp)(bottom_data[n]);\n      const int_tp weight_index = index * N + d;\n      top_data[top_index] = weight[weight_index];\n    }\n  }\n\n// atomic_add from: http://suhorukov.blogspot.com/2011/12/opencl-11-atomic-operations-on-floating.html\n#if (TYPE == TYPE_FLOAT)\ninline void TEMPLATE(atomic_add,Dtype)(volatile __global Dtype *source, const Dtype operand) {\n    union {\n        uint_tp intVal;\n        Dtype floatVal;\n    } newVal;\n    union {\n        uint_tp intVal;\n        Dtype floatVal;\n    } prevVal;\n    do {\n        prevVal.floatVal = *source;\n        newVal.floatVal = prevVal.floatVal + operand;\n    } while (atomic_cmpxchg((volatile __global unsigned int *)source, prevVal.intVal, newVal.intVal) != prevVal.intVal);\n}\n\n__kernel void TEMPLATE(embed_backward,Dtype)(const int_tp nthreads, __global const Dtype* bottom_data,\n    __global const Dtype* top_diff, const int_tp M, const int_tp N, const int_tp K,\n    __global Dtype* weight_diff) {\n  for (int_tp top_index = get_global_id(0); top_index < nthreads;\n      top_index += get_global_size(0)) {\n    const int_tp n = top_index / N;\n    const int_tp d = top_index % N;\n    const int_tp index = (int_tp)(bottom_data[n]);\n    const int_tp weight_index = index * N + d;\n\n    TEMPLATE(atomic_add,Dtype)((weight_diff + weight_index), *(top_diff + top_index));\n  }\n}\n#endif\n\n#if (TYPE == TYPE_DOUBLE)\n#ifdef ATOMICS_64_AVAILABLE\ninline void TEMPLATE(atomic_add,Dtype)(volatile __global Dtype *source, const Dtype operand) {\n    union {\n        unsigned long intVal;\n        Dtype floatVal;\n    } newVal;\n    union {\n        unsigned long intVal;\n        Dtype floatVal;\n    } prevVal;\n    do {\n        prevVal.floatVal = *source;\n        newVal.floatVal = prevVal.floatVal + operand;\n    } while (atom_cmpxchg((volatile __global unsigned long *)source, prevVal.intVal, newVal.intVal) != prevVal.intVal);\n}\n\n__kernel void TEMPLATE(embed_backward,Dtype)(const int_tp nthreads, __global const Dtype* bottom_data,\n    __global const Dtype* top_diff, const int_tp M, const int_tp N, const int_tp K,\n    __global Dtype* weight_diff) {\n  for (int_tp top_index = get_global_id(0); top_index < nthreads;\n      top_index += get_global_size(0)) {\n    const int_tp n = top_index / N;\n    const int_tp d = top_index % N;\n    const int_tp index = (int_tp)(bottom_data[n]);\n    const int_tp weight_index = index * N + d;\n\n    TEMPLATE(atomic_add,Dtype)((weight_diff + weight_index), *(top_diff + top_index));\n  }\n}\n#endif\n#endif";  // NOLINT
std::string fillbuffer_double = "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(fillbuffer,Dtype)(const int_tp n, const char alpha, __global char* x,\n                                   const int_tp offx) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    x[index + offx] = alpha;\n  }\n}\n\n__kernel void TEMPLATE(fill,Dtype)(const int_tp n, const Dtype alpha, __global Dtype* x,\n                                   const int_tp offx) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    x[index + offx] = alpha;\n  }\n}";  // NOLINT
std::string im2col_double = "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(im2col,Dtype)(const int_tp n, __global const Dtype* data_im, const int_tp data_im_off,\n    const int_tp height, const int_tp width, const int_tp kernel_h, const int_tp kernel_w,\n    const int_tp pad_h, const int_tp pad_w,\n    const int_tp stride_h, const int_tp stride_w,\n    const int_tp height_col, const int_tp width_col,\n    __global Dtype* data_col, const int_tp data_col_off) {\n\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    int_tp w_out = index % width_col;\n    int_tp h_index = index / width_col;\n    int_tp h_out = h_index % height_col;\n    int_tp channel_in = h_index / height_col;\n    int_tp channel_out = channel_in * kernel_h * kernel_w;\n    int_tp h_in = h_out * stride_h - pad_h;\n    int_tp w_in = w_out * stride_w - pad_w;\n    __global Dtype* data_col_ptr = data_col + data_col_off;\n    data_col_ptr += (channel_out * height_col + h_out) * width_col + w_out;\n    __global const Dtype* data_im_ptr = data_im + data_im_off;\n    data_im_ptr += (channel_in * height + h_in) * width + w_in;\n    for (int_tp i = 0; i < kernel_h; ++i) {\n      for (int_tp j = 0; j < kernel_w; ++j) {\n        int_tp h = h_in + i;\n        int_tp w = w_in + j;\n        *data_col_ptr = (h >= 0 && w >= 0 && h < height && w < width) ?\n            data_im_ptr[i * width + j] : 0;\n        data_col_ptr += height_col * width_col;\n      }\n    }\n  }\n}\n\n__kernel void TEMPLATE(col2im,Dtype)(const int_tp n, __global const Dtype* data_col, const int_tp data_col_off,\n    const int_tp height, const int_tp width, const int_tp channels,\n    const int_tp patch_h, const int_tp patch_w,\n    const int_tp pad_h, const int_tp pad_w,\n    const int_tp stride_h, const int_tp stride_w,\n    const int_tp height_col, const int_tp width_col,\n    __global Dtype* data_im, const int_tp data_im_off) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    Dtype val = 0;\n    int_tp w = index % width + pad_w;\n    int_tp h = (index / width) % height + pad_h;\n    int_tp c = index / (width * height);\n    // compute the start and end of the output\n    int_tp w_col_start = (w < patch_w) ? 0 : (w - patch_w) / stride_w + 1;\n    int_tp w_col_end = min(w / stride_w + 1, width_col);\n    int_tp h_col_start = (h < patch_h) ? 0 : (h - patch_h) / stride_h + 1;\n    int_tp h_col_end = min(h / stride_h + 1, height_col);\n    int_tp offset = data_col_off +\n        (c * patch_h * patch_w + h * patch_w + w) * height_col * width_col;\n    int_tp coeff_h_col = (1 - stride_h * patch_w * height_col) * width_col;\n    int_tp coeff_w_col = (1 - stride_w * height_col * width_col);\n    for (int_tp h_col = h_col_start; h_col < h_col_end; ++h_col) {\n      for (int_tp w_col = w_col_start; w_col < w_col_end; ++w_col) {\n        val += data_col[offset + h_col * coeff_h_col + w_col * coeff_w_col];\n      }\n    }\n    data_im[index + data_im_off] = val;\n  }\n}";  // NOLINT
std::string im2col_nd_double = "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(im2col_nd, Dtype)(const int_tp n, const int_tp num_axes,\n                                     const int_tp channel_axis,\n                                     __global const Dtype* data_im,\n                                     const int_tp data_off,\n                                     __global const int_tp* im_shape,\n                                     __global const int_tp* col_shape,\n                                     __global const int_tp* kernel_shape,\n                                     __global const int_tp* pad,\n                                     __global const int_tp* stride,\n                                     __global Dtype* data_col,\n                                     const int_tp data_col_off) {\n\n  int_tp d_temp[6];\n  int_tp d_iter[6];\n  int_tp i;\n\n  __global const int_tp* im_shape_ptr = im_shape + channel_axis;\n  __global const int_tp* col_shape_ptr = col_shape + channel_axis;\n\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    // Initialize channel_in, computed in the loop below, with intermediate\n    // computations used to compute the spatial indices.\n    int_tp channel_in = index;\n    int_tp channel_out = 1;\n    for (i = num_axes - 1; i >= 0; --i) {\n      d_temp[i] = channel_in % col_shape_ptr[i + 1];\n      channel_in /= col_shape_ptr[i + 1];\n      channel_out *= kernel_shape[i];\n    }\n    channel_out *= channel_in;\n    int_tp data_col_inc = 1;\n    for (i = 0; i < num_axes; ++i) {\n      channel_out *= col_shape_ptr[i + 1];\n      channel_out += d_temp[i];\n      d_temp[i] = d_temp[i] * stride[i] - pad[i];\n      channel_in *= im_shape_ptr[i + 1];\n      channel_in += d_temp[i];\n      data_col_inc *= col_shape_ptr[i + 1];\n      d_iter[i] = 0;\n    }\n    __global Dtype* data_col_ptr = data_col + data_col_off + channel_out;\n    __global const Dtype* data_im_ptr = data_im + data_off + channel_in;\n    bool incremented;\n    do {\n      bool in_range = true;\n      for (i = 0; i < num_axes; ++i) {\n        const int_tp d_iter_im = d_iter[i] + d_temp[i];\n        in_range &= d_iter_im >= 0 && d_iter_im < im_shape_ptr[i + 1];\n        if (!in_range) {\n          break;\n        }\n      }\n      if (in_range) {\n        int_tp data_im_offset = d_iter[0];\n        for (i = 1; i < num_axes; ++i) {\n          data_im_offset *= im_shape_ptr[i + 1];\n          data_im_offset += d_iter[i];\n        }\n        *data_col_ptr = data_im_ptr[data_im_offset];\n      } else {\n        *data_col_ptr = 0;\n      }\n      data_col_ptr += data_col_inc;\n      incremented = false;\n      for (i = num_axes - 1; i >= 0; --i) {\n        const int_tp d_max = kernel_shape[i];\n        if (d_iter[i] == d_max - 1) {\n          d_iter[i] = 0;\n        } else {  // d_iter[i] < d_max - 1\n          ++d_iter[i];\n          incremented = true;\n          break;\n        }\n      }  // for (int_tp i = num_axes - 1; i >= 0; --i)\n    } while (incremented);  // do\n  }\n}\n\n\n\n__kernel void TEMPLATE(col2im_nd, Dtype)(const int_tp n, const int_tp num_axes,\n                                         const int_tp channel_axis,\n                                         __global const Dtype* data_col,\n                                         const int_tp data_col_off,\n                                         __global const int_tp* im_shape,\n                                         __global const int_tp* col_shape,\n                                         __global const int_tp* kernel_shape,\n                                         __global const int_tp* pad,\n                                         __global const int_tp* stride,\n                                         __global Dtype* data_im,\n                                         const int_tp data_im_off) {\n  int_tp d_im[6];\n  int_tp d_col_iter[6];\n  int_tp d_col_start[6];\n  int_tp d_col_end[6];\n\n  __global const int_tp* im_shape_ptr = im_shape + channel_axis;\n  __global const int_tp* col_shape_ptr = col_shape + channel_axis;\n  __global Dtype* data_col_ptr = data_col + data_col_off;\n\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    // Initialize channel_in, computed in the loop below, with intermediate\n    // computations used to compute the spatial indices.\n    int_tp channel_im = index;\n    // Calculate d_im (image dimensions).\n    for (int_tp i = num_axes - 1; i >= 0; --i) {\n      d_im[i] = channel_im % im_shape_ptr[i + 1] + pad[i];\n      channel_im /= im_shape_ptr[i + 1];\n    }\n    // Calculate col start/end indices.\n    bool done = false;\n    for (int_tp i = 0; i < num_axes; ++i) {\n      d_col_start[i] = d_col_iter[i] =\n          (d_im[i] < kernel_shape[i]) ?\n              0 : (d_im[i] - kernel_shape[i]) / stride[i] + 1;\n      d_col_end[i] = min(d_im[i] / stride[i] + 1, col_shape_ptr[i + 1]);\n      if (d_col_start[i] >= d_col_end[i]) {\n        // Skip computation if the dimension is 0 at any spatial axis --\n        // final val will be 0.\n        data_im[index + data_im_off] = 0;\n        done = true;\n        break;  // for (int_tp i = 0; i < num_axes; ++i)\n      }\n    }\n    if (done) {\n      continue;\n    }\n    // Loop over the col to compute the output val.\n    Dtype val = 0;\n    bool incremented = true;\n    do {\n      // Compute the final offset.\n      int_tp final_offset = 0;\n      int_tp kernel_shape_prod = 1;\n      for (int_tp i = num_axes - 1; i >= 0; --i) {\n        final_offset += (d_im[i] - d_col_iter[i] * stride[i])\n            * kernel_shape_prod;\n        kernel_shape_prod *= kernel_shape[i];\n      }\n      final_offset += kernel_shape_prod * channel_im;\n      for (int_tp i = 0; i < num_axes; ++i) {\n        final_offset *= col_shape_ptr[i + 1];\n        final_offset += d_col_iter[i];\n      }\n      val += data_col_ptr[final_offset];\n      incremented = false;\n      for (int_tp i = num_axes - 1; i >= 0; --i) {\n        const int_tp d_max = d_col_end[i];\n        if (d_col_iter[i] == d_max - 1) {\n          d_col_iter[i] = d_col_start[i];\n        } else {  // d_col_iter[i] < d_max - 1\n          ++d_col_iter[i];\n          incremented = true;\n          break;  // for (int_tp i = num_axes - 1; i >= 0; --i)\n        }\n      }  // for (int_tp i = num_axes - 1; i >= 0; --i)\n    } while (incremented);\n    data_im[index + data_im_off] = val;\n  }\n}";  // NOLINT
std::string im2col_ndsk_double = "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(im2col_ndsk, Dtype)(const int_tp n, const int_tp num_axes,\n                                        __global const Dtype* data_im,\n                                        const int_tp data_off,\n                                        __global const int_tp* im_shape,\n                                        __global const int_tp* col_shape,\n                                        __global const int_tp* kernel_shape,\n                                        __global const int_tp* pad,\n                                        __global const int_tp* stride,\n                                        __global const int_tp* kstride,\n                                        __global Dtype* data_col,\n                                        const int_tp data_col_off) {\n  int_tp d_temp[6];\n  int_tp d_iter[6];\n  int_tp i;\n\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    // Initialize channel_in, computed in the loop below, with intermediate\n    // computations used to compute the spatial indices.\n    int_tp channel_in = index;\n    int_tp channel_out = 1;\n    for (i = num_axes - 1; i >= 0; --i) {\n      d_temp[i] = channel_in % col_shape[i + 1];\n      channel_in /= col_shape[i + 1];\n      channel_out *= kernel_shape[i];\n    }\n    channel_out *= channel_in;\n    int_tp data_col_inc = 1;\n    for (i = 0; i < num_axes; ++i) {\n      channel_out *= col_shape[i + 1];\n      channel_out += d_temp[i];\n      d_temp[i] = d_temp[i] * stride[i] - pad[i];\n      channel_in *= im_shape[i + 1];\n      channel_in += d_temp[i];\n      data_col_inc *= col_shape[i + 1];\n      d_iter[i] = 0;\n    }\n    __global Dtype* data_col_ptr = data_col + data_col_off + channel_out;\n    __global const Dtype* data_im_ptr = data_im + data_off + channel_in;\n    bool incremented;\n    do {\n      bool in_range = true;\n      for (i = 0; i < num_axes; ++i) {\n        const int_tp d_iter_im = d_iter[i] + d_temp[i];\n        in_range &= d_iter_im >= 0 && d_iter_im < im_shape[i + 1];\n        if (!in_range) {\n          break;\n        }\n      }\n\n      // Write column data\n      if (in_range) {\n        int_tp data_im_offset = d_iter[0];\n        for (i = 1; i < num_axes; ++i) {\n          data_im_offset *= im_shape[i + 1];\n          data_im_offset += d_iter[i];\n        }\n        *data_col_ptr = data_im_ptr[data_im_offset];\n      } else {\n        *data_col_ptr = 0;\n      }\n\n      data_col_ptr += data_col_inc;\n      incremented = false;\n      for (i = num_axes - 1; i >= 0; --i) {\n        // Old: const int_tp d_max = kernel_shape[i];\n        // New (strided, limit is the external kernel size):\n        const int_tp d_max = (kernel_shape[i] - 1) * kstride[i] + 1;\n        if (d_iter[i] == d_max - 1) {\n          d_iter[i] = 0;\n        } else {  // d_iter[i] < d_max - 1\n          // Old: ++d_iter[i];\n          // New (strided, increment by the stride each time):\n          d_iter[i] += kstride[i];\n          incremented = true;\n          break;\n        }\n      }  // for (int_tp i = num_axes - 1; i >= 0; --i)\n    } while (incremented);  // do\n  }\n}\n\n__kernel void TEMPLATE(col2im_ndsk, Dtype)(const int_tp n, const int_tp num_axes,\n                                  __global const Dtype* data_col,\n                                    const int_tp data_col_off,\n                                  __global const int_tp* im_shape,\n                                  __global const int_tp* col_shape,\n                                  __global const int_tp* kernel_shape,\n                                  __global const int_tp* pad,\n                                  __global const int_tp* stride,\n                                  __global const int_tp* kstride,\n                                  __global Dtype* data_im,\n                                  const int_tp data_off) {\n  int_tp d_im[6];\n  int_tp d_col_size[6];\n  int_tp d_col_iter[6];\n  int_tp d_col_start[6];\n  int_tp d_col_end[6];\n  int_tp d_ext_patch[6];\n  int_tp d_idx[6];\n\n  for (int_tp i = num_axes - 1; i >= 0; --i) {\n    d_ext_patch[i] = (kernel_shape[i] - 1) * kstride[i] + 1;\n    d_col_size[i] = (im_shape[i + 1] + 2 * pad[i] - d_ext_patch[i])\n        / stride[i] + 1;\n  }\n\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    // Initialize channel_in, computed in the loop below, with intermediate\n    // computations used to compute the spatial indices.\n    int_tp channel_im = index;\n    // Calculate d_im (image dimensions).\n    for (int_tp i = num_axes - 1; i >= 0; --i) {\n      d_im[i] = channel_im % im_shape[i + 1] + pad[i];\n      channel_im /= im_shape[i + 1];\n    }\n    // Calculate col start/end indices.\n    bool done = false;\n    for (int_tp i = 0; i < num_axes; ++i) {\n      // Old:\n      /*d_col_start[i] = d_col_iter[i] =\n          (d_im[i] < kernel_shape[i]) ?\n          0 : (d_im[i] - kernel_shape[i]) / stride[i] + 1;\n      d_col_end[i] = min(d_im[i] / stride[i] + 1, col_shape[i + 1]);*/\n      // New:\n      d_col_start[i] = (d_im[i] < d_ext_patch[i]) ?\n          d_im[i] % kstride[i] : (d_im[i] - d_ext_patch[i]) + 1;\n      d_col_iter[i] = d_col_start[i];\n      d_idx[i] = (d_im[i] - d_col_start[i]) / kstride[i];\n      d_col_end[i] = (d_im[i] >= d_col_size[i]) ?\n          (d_col_size[i] - 1) - ((d_col_size[i] - 1) - d_col_start[i])\n          % kstride[i] : d_im[i];\n      if (d_col_start[i] > d_col_end[i]) {\n        // Skip computation if the dimension is 0 at any spatial axis --\n        // final val will be 0.\n        data_im[index] = 0;\n        done = true;\n        break;  // for (int_tp i = 0; i < num_axes; ++i)\n      }\n    }\n    if (done) {\n      continue;\n    }\n    // Loop over the col to compute the output val.\n    Dtype val = 0;\n    bool incremented = true;\n    do {\n      // Compute the final offset.\n      int_tp final_offset = 0;\n      int_tp coeff_prod = 1;\n      for (int_tp i = num_axes - 1; i >= 0; --i) {\n        final_offset +=  d_col_iter[i] * coeff_prod;\n        coeff_prod *= d_col_size[i];\n      }\n      for (int_tp i = num_axes - 1; i >= 0; --i) {\n        final_offset += d_idx[i] * coeff_prod;\n        coeff_prod *= kernel_shape[i];\n      }\n      final_offset += channel_im * coeff_prod;\n      val += data_col[final_offset];\n      incremented = false;\n      for (int_tp i = num_axes - 1; i >= 0; --i) {\n        if (d_col_iter[i] > d_col_end[i] - kstride[i]) {\n          d_col_iter[i] = d_col_start[i];\n          d_idx[i] = (d_im[i] - d_col_start[i]) / kstride[i];\n        } else {  // d_col_iter[i] <= d_max - kstride[1]\n          d_col_iter[i] += kstride[i];\n          --d_idx[i];\n          incremented = true;\n          break;  // for (int_tp i = num_axes - 1; i >= 0; --i)\n        }\n      }  // for (int_tp i = num_axes - 1; i >= 0; --i)\n    }  while (incremented);\n    data_im[index] = val;\n  }\n}";  // NOLINT
std::string im2col_sk_double = "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(im2col_sk,Dtype)(const int_tp n,\n                                        __global const Dtype* data_im,\n                                        const int_tp data_offset, const int_tp height,\n                                        const int_tp width, const int_tp kernel_h,\n                                        const int_tp kernel_w,\n                                        const int_tp ext_kernel_h,\n                                        const int_tp ext_kernel_w, const int_tp pad_h,\n                                        const int_tp pad_w, const int_tp stride_h,\n                                        const int_tp stride_w, const int_tp kstride_h,\n                                        const int_tp kstride_w,\n                                        const int_tp height_col,\n                                        const int_tp width_col,\n                                        __global Dtype* data_col) {\n\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    int_tp w_out = index % width_col;\n    int_tp h_index = index / width_col;\n    int_tp h_out = h_index % height_col;\n    int_tp channel_in = h_index / height_col;\n    int_tp channel_out = channel_in * kernel_h * kernel_w;\n    int_tp h_in = h_out * stride_h - pad_h;\n    int_tp w_in = w_out * stride_w - pad_w;\n    __global Dtype* data_col_ptr = data_col;\n    data_col_ptr += (channel_out * height_col + h_out) * width_col + w_out;\n    __global const Dtype* data_im_ptr = data_im + data_offset;\n    data_im_ptr += (channel_in * height + h_in) * width + w_in;\n    for (int_tp i = 0; i < ext_kernel_h; i += kstride_h) {\n      for (int_tp j = 0; j < ext_kernel_w; j += kstride_w) {\n        int_tp h = h_in + i;\n        int_tp w = w_in + j;\n        (*data_col_ptr) =\n            (h >= 0 && w >= 0 && h < height && w < width) ?\n                data_im_ptr[i * width + j] : 0;\n        data_col_ptr += height_col * width_col;\n      }\n    }\n  }\n\n}\n\n__kernel void TEMPLATE(col2im_sk,Dtype)(const int_tp n,\n                                        __global const Dtype* data_col,\n                                        const int_tp height, const int_tp width,\n                                        const int_tp channels, const int_tp patch_h,\n                                        const int_tp patch_w,\n                                        const int_tp ext_patch_h,\n                                        const int_tp ext_patch_w, const int_tp pad_h,\n                                        const int_tp pad_w, const int_tp stride_h,\n                                        const int_tp stride_w, const int_tp kstride_h,\n                                        const int_tp kstride_w,\n                                        const int_tp height_col,\n                                        const int_tp width_col,\n                                        __global Dtype* data_im,\n                                        const int_tp data_offset) {\n\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    Dtype val = 0;\n    int_tp w = index % width + pad_w;\n    int_tp h = (index / width) % height + pad_h;\n    int_tp c = index / (width * height);\n    // compute the start and end of the output\n    int_tp width_col_1 = width_col - 1;\n    int_tp height_col_1 = height_col - 1;\n    int_tp w_col_start = (w < ext_patch_w) ? w % kstride_w : (w - ext_patch_w) + 1;\n    int_tp w_col_end =\n        (w >= width_col) ?\n            width_col_1 - (width_col_1 - w_col_start) % kstride_w : w;\n    int_tp h_col_start = (h < ext_patch_h) ? h % kstride_h : (h - ext_patch_h) + 1;\n    int_tp h_col_end =\n        (h >= height_col) ?\n            height_col_1 - (height_col_1 - h_col_start) % kstride_h : h;\n    int_tp w_num = (w - w_col_start) / kstride_w;\n    int_tp h_num = (h - h_col_start) / kstride_h;\n\n    int_tp coeff_w_idx = height_col * width_col;\n    int_tp coeff_h_idx = patch_w * coeff_w_idx;\n    int_tp offset = c * patch_h * coeff_h_idx;\n    for (int_tp h_col = h_col_start, h_idx = h_num; h_col <= h_col_end; h_col +=\n        kstride_h, --h_idx) {\n      for (int_tp w_col = w_col_start, w_idx = w_num; w_col <= w_col_end; w_col +=\n          kstride_w, --w_idx) {\n        //int_tp c_col = c * patch_h * patch_w + (h - h_col) / kstride_h * patch_w + (w - w_col) / kstride_w;\n        //int_tp c_col = c * patch_h * patch_w + h_idx * patch_w + w_idx;\n        //val += data_col[(c_col * height_col + h_col) * width_col + w_col];\n        val += data_col[offset + h_idx * coeff_h_idx + w_idx * coeff_w_idx\n            + h_col * width_col + w_col];\n      }\n    }\n\n    data_im[data_offset + index] = val;\n  }\n}";  // NOLINT
std::string lrn_double = "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(lrn_compute_output,Dtype)(const int_tp nthreads,\n                                                 __global const Dtype* in,\n                                                 __global const Dtype* scale,\n                                                 const Dtype negative_beta,\n                                                 __global Dtype* out) {\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    out[index] = in[index] * pow(scale[index], negative_beta);\n  }\n}\n\n__kernel void TEMPLATE(lrn_fill_scale,Dtype)(const int_tp nthreads, __global const Dtype* in,\n                             const int_tp num, const int_tp channels,\n                             const int_tp height, const int_tp width, const int_tp size,\n                             const Dtype alpha_over_size, const Dtype k,\n                             __global Dtype* const scale) {\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    // find out the local offset\n    const int_tp w = index % width;\n    const int_tp h = (index / width) % height;\n    const int_tp n = index / width / height;\n    const int_tp offset = (n * channels * height + h) * width + w;\n    const int_tp step = height * width;\n    __global const Dtype* in_off = in + offset;\n    __global Dtype* scale_off = scale + offset;\n    int_tp head = 0;\n    const int_tp pre_pad = (size - 1) / 2;\n    const int_tp post_pad = size - pre_pad - 1;\n    Dtype accum_scale = 0;\n    // fill the scale at [n, :, h, w]\n    // accumulate values\n    while (head < post_pad && head < channels) {\n      accum_scale += in_off[head * step] * in_off[head * step];\n      ++head;\n    }\n    // both add and subtract\n    while (head < channels) {\n      accum_scale += in_off[head * step] * in_off[head * step];\n      if (head - size >= 0) {\n        accum_scale -= in_off[(head - size) * step]\n            * in_off[(head - size) * step];\n      }\n      scale_off[(head - post_pad) * step] = k + accum_scale * alpha_over_size;\n      ++head;\n    }\n    // subtract only\n    while (head < channels + post_pad) {\n      if (head - size >= 0) {\n        accum_scale -= in_off[(head - size) * step]\n            * in_off[(head - size) * step];\n      }\n      scale_off[(head - post_pad) * step] = k + accum_scale * alpha_over_size;\n      ++head;\n    }\n  }\n}\n\n__kernel void TEMPLATE(lrn_compute_diff,Dtype)(const int_tp nthreads,\n                               __global const Dtype* bottom_data,\n                               __global const Dtype* top_data,\n                               __global const Dtype* scale,\n                               __global const Dtype* top_diff, const int_tp num,\n                               const int_tp channels, const int_tp height,\n                               const int_tp width, const int_tp size,\n                               const Dtype negative_beta,\n                               const Dtype cache_ratio,\n                               __global Dtype* bottom_diff) {\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    // find out the local offset\n    const int_tp w = index % width;\n    const int_tp h = (index / width) % height;\n    const int_tp n = index / width / height;\n    const int_tp offset = (n * channels * height + h) * width + w;\n    const int_tp step = height * width;\n    __global const Dtype* bottom_off = bottom_data + offset;\n    __global const Dtype* top_off = top_data + offset;\n    __global const Dtype* scale_off = scale + offset;\n    __global Dtype* top_diff_off = top_diff + offset;\n    __global Dtype* bottom_diff_off = bottom_diff + offset;\n    int_tp head = 0;\n    const int_tp pre_pad = size - (size + 1) / 2;\n    const int_tp post_pad = size - pre_pad - 1;\n    Dtype accum_ratio = 0;\n    // accumulate values\n    while (head < post_pad && head < channels) {\n      accum_ratio += top_diff_off[head * step] * top_off[head * step]\n          / scale_off[head * step];\n      ++head;\n    }\n    // both add and subtract\n    while (head < channels) {\n      accum_ratio += top_diff_off[head * step] * top_off[head * step]\n          / scale_off[head * step];\n      if (head - size >= 0) {\n        accum_ratio -= top_diff_off[(head - size) * step]\n            * top_off[(head - size) * step] / scale_off[(head - size) * step];\n      }\n      bottom_diff_off[(head - post_pad) * step] = top_diff_off[(head - post_pad)\n          * step] * pow(scale_off[(head - post_pad) * step], negative_beta)\n          - cache_ratio * bottom_off[(head - post_pad) * step] * accum_ratio;\n      ++head;\n    }\n    // subtract only\n    while (head < channels + post_pad) {\n      if (head - size >= 0) {\n        accum_ratio -= top_diff_off[(head - size) * step]\n            * top_off[(head - size) * step] / scale_off[(head - size) * step];\n      }\n      bottom_diff_off[(head - post_pad) * step] = top_diff_off[(head - post_pad)\n          * step] * pow(scale_off[(head - post_pad) * step], negative_beta)\n          - cache_ratio * bottom_off[(head - post_pad) * step] * accum_ratio;\n      ++head;\n    }\n  }\n}";  // NOLINT
std::string math_double = "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(mul,Dtype)(const int_tp n, __global const Dtype* a,\n                                  const int_tp offa,\n                                  __global Dtype* b,\n                                  const int_tp offb, __global Dtype* y,\n                                  const int_tp offy) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    y[index + offy] = a[index + offa] * b[index + offb];\n  }\n}\n\n__kernel void TEMPLATE(div,Dtype)(const int_tp n, __global const Dtype* a,\n                                  const int_tp offa,\n                                  __global Dtype* b,\n                                  const int_tp offb, __global Dtype* y,\n                                  const int_tp offy) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    y[index + offy] = a[index + offa] / b[index + offb];\n  }\n}\n\n__kernel void TEMPLATE(add_scalar,Dtype)(const int_tp N, const Dtype alpha,\n__global Dtype* Y,\n                                         const int_tp offY) {\n  for (int_tp index = get_global_id(0); index < N; index += get_global_size(0)) {\n    Y[offY + index] += alpha;\n  }\n}\n\n__kernel void TEMPLATE(add,Dtype)(const int_tp n, __global const Dtype* a,\n                                  const int_tp offa, __global const Dtype* b,\n                                  const int_tp offb, __global Dtype* y,\n                                  const int_tp offy) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    y[offy + index] = a[offa + index] + b[offb + index];\n  }\n}\n\n__kernel void TEMPLATE(sub,Dtype)(const int_tp n, __global const Dtype* a,\n                                  const int_tp offa, __global const Dtype* b,\n                                  const int_tp offb, __global Dtype* y,\n                                  const int_tp offy) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    y[offy + index] = a[offa + index] - b[offb + index];\n  }\n}\n\n__kernel void TEMPLATE(abs,Dtype)(const int_tp n, __global const Dtype* a,\n                                  const int_tp offa, __global Dtype* y,\n                                  const int_tp offy) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    y[offy + index] = fabs((Dtype)(a[offa + index]));\n  }\n}\n\n__kernel void TEMPLATE(exp,Dtype)(const int_tp n, __global const Dtype* a,\n                                  const int_tp offa, __global Dtype* y,\n                                  const int_tp offy) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    y[offy + index] = exp(a[offa + index]);\n  }\n}\n\n__kernel void TEMPLATE(log,Dtype)(const int_tp n, __global const Dtype* a,\n                                  const int_tp offa, __global Dtype* y,\n                                  const int_tp offy) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    y[offy + index] = log(a[offa + index]);\n  }\n}\n\n__kernel void TEMPLATE(powx,Dtype)(const int_tp n, __global const Dtype* a,\n                                   const int_tp offa, Dtype alpha,\n                                   __global Dtype* y,\n                                   const int_tp offy) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    if(alpha == 2.0) {\n      y[offy + index] = pow((Dtype)fabs(a[offa + index]), (Dtype)alpha);\n    } else {\n      y[offy + index] = pow((Dtype)a[offa + index], (Dtype)alpha);\n    }\n  }\n}\n\n__kernel void TEMPLATE(sign,Dtype)(const int_tp n, __global const Dtype* x,\n                                   const int_tp offx, __global Dtype* y,\n                                   const int_tp offy) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    y[index + offy] = (0.0 < x[index + offx])\n        - (x[index + offx] < 0.0);\n  }\n}\n\n__kernel void TEMPLATE(sgnbit,Dtype)(const int_tp n, __global const Dtype* x,\n                                     const int_tp offx, __global Dtype* y,\n                                     const int_tp offy) {\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    y[index + offy] = signbit(x[index + offx]);\n  }\n}";  // NOLINT
std::string mergecrop_double = "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(merge_copy_forward, Dtype)(const int_tp nthreads,\n                                                  const int_tp dims,\n                                                  __global const Dtype* bottom_a,\n                                                  const int_tp forward_a,\n                                                  __global const Dtype* bottom_b,\n                                                  const int_tp forward_b,\n                                                  __global Dtype* top,\n                                                  const int_tp num,\n                                                  const int_tp channels_a,\n                                                  const int_tp channels_b,\n                                                  __global const int_tp* shape_a,\n                                                  __global const int_tp* shape_b) {\n  int_tp pad[6];\n  int_tp tmp_idx[6];\n  int_tp size_a = 1;\n  int_tp size_b = 1;\n\n  for (int_tp i = 0; i < dims; ++i) {\n    pad[i] = (shape_b[i] - shape_a[i]) / 2;\n    size_a *= shape_a[i];\n    size_b *= shape_b[i];\n  }\n\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    int_tp batch_id = index / ((channels_a + channels_b) * size_a);\n    int_tp bottom_id = ((index - batch_id * (channels_a + channels_b) * size_a)\n        / (channels_a * size_a)) % 2;\n    int_tp counter = index;\n    for (int_tp i = dims - 1; i >= 0; --i) {\n      tmp_idx[i] = counter % shape_a[i];\n      counter /= shape_a[i];\n    }\n\n    if (bottom_id == 0) {\n      int_tp channel_id = (index / size_a) % channels_a;\n      int_tp aidx = batch_id * channels_a + channel_id;\n      for (int_tp i = 0; i < dims; ++i) {\n        aidx *= shape_a[i];\n        aidx += tmp_idx[i];\n      }\n      top[index] = (forward_a == 1) ? bottom_a[aidx] : 0;\n    } else {\n      int_tp channel_id = (index / size_a) % channels_b;\n      int_tp bidx = (batch_id * channels_b + channel_id) * size_b;\n      int_tp btemp = 1;\n      for (int_tp i = dims - 1; i >= 0; --i) {\n        bidx += btemp * (tmp_idx[i] + pad[i]);\n        btemp *= shape_b[i];\n      }\n      top[index] = (forward_b == 1) ? bottom_b[bidx] : 0;\n    }\n  }\n}\n\n__kernel void TEMPLATE(merge_copy_backward,Dtype)(const int_tp nthreads,\n                                                  const int_tp dims,\n                                                  __global Dtype* bottom_a,\n                                                  const int_tp backward_a,\n                                                  __global Dtype* bottom_b,\n                                                  const int_tp backward_b,\n                                                  __global const Dtype* top,\n                                                  const int_tp num,\n                                                  const int_tp channels_a,\n                                                  const int_tp channels_b,\n                                                  __global const int_tp* shape_a,\n                                                  __global const int_tp* shape_b) {\n  int_tp pad[6];\n  int_tp tmp_idx[6];\n  int_tp size_a = 1;\n  int_tp size_b = 1;\n\n  for (int_tp i = 0; i < dims; ++i) {\n    pad[i] = (shape_b[i] - shape_a[i]) / 2;\n    size_a *= shape_a[i];\n    size_b *= shape_b[i];\n  }\n\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    int_tp batch_id = index / ((channels_a + channels_b) * size_a);\n    int_tp bottom_id = ((index - batch_id * (channels_a + channels_b) * size_a)\n        / (channels_a * size_a)) % 2;\n    int_tp counter = index;\n    for (int_tp i = dims - 1; i >= 0; --i) {\n      tmp_idx[i] = counter % shape_a[i];\n      counter /= shape_a[i];\n    }\n\n    if (bottom_id == 0) {\n      int_tp channel_id = (index / size_a) % channels_a;\n      int_tp aidx = batch_id * channels_a + channel_id;\n      for (int_tp i = 0; i < dims; ++i) {\n        aidx *= shape_a[i];\n        aidx += tmp_idx[i];\n      }\n      bottom_a[aidx] = (backward_a == 1) ? top[index] : 0;\n    } else {\n      int_tp channel_id = (index / size_a) % channels_b;\n      int_tp bidx = (batch_id * channels_b + channel_id) * size_b;\n      int_tp btemp = 1;\n      for (int_tp i = dims - 1; i >= 0; --i) {\n        bidx += btemp * (tmp_idx[i] + pad[i]);\n        btemp *= shape_b[i];\n      }\n      bottom_b[bidx] = (backward_b == 1) ? top[index] : 0;\n    }\n  }\n}";  // NOLINT
std::string pooling_double = "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(max_pool_forward,Dtype)(\n    const int_tp nthreads, __global const Dtype* bottom_data, const int_tp num,\n    const int_tp channels, const int_tp height, const int_tp width,\n    const int_tp pooled_height, const int_tp pooled_width, const int_tp kernel_h,\n    const int_tp kernel_w, const int_tp stride_h, const int_tp stride_w, const int_tp pad_h,\n    const int_tp pad_w,\n    __global Dtype* top_data,\n    const int use_mask, __global int_tp* mask, __global Dtype* top_mask) {\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    const int_tp pw = index % pooled_width;\n    const int_tp ph = (index / pooled_width) % pooled_height;\n    const int_tp c = (index / pooled_width / pooled_height) % channels;\n    const int_tp n = index / pooled_width / pooled_height / channels;\n    int_tp hstart = ph * stride_h - pad_h;\n    int_tp wstart = pw * stride_w - pad_w;\n    const int_tp hend = min(hstart + kernel_h, height);\n    const int_tp wend = min(wstart + kernel_w, width);\n    hstart = max(hstart, 0L);\n    wstart = max(wstart, 0L);\n    Dtype maxval = -FLT_MAX;\n    int_tp maxidx = -1;\n    __global const Dtype* bottom_slice = bottom_data\n        + (n * channels + c) * height * width;\n    for (int_tp h = hstart; h < hend; ++h) {\n      for (int_tp w = wstart; w < wend; ++w) {\n        if (bottom_slice[h * width + w] > maxval) {\n          maxidx = h * width + w;\n          maxval = bottom_slice[maxidx];\n        }\n      }\n    }\n    top_data[index] = maxval;\n    if (use_mask == 1) {\n      mask[index] = maxidx;\n    } else {\n      top_mask[index] = maxidx;\n    }\n  }\n}\n\n__kernel void TEMPLATE(ave_pool_forward,Dtype)(\n    const int_tp nthreads, __global const Dtype* const bottom_data, const int_tp num,\n    const int_tp channels, const int_tp height, const int_tp width,\n    const int_tp pooled_height, const int_tp pooled_width, const int_tp kernel_h,\n    const int_tp kernel_w, const int_tp stride_h, const int_tp stride_w, const int_tp pad_h,\n    const int_tp pad_w, __global Dtype* top_data) {\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    {\n      const int_tp pw = index % pooled_width;\n      const int_tp ph = (index / pooled_width) % pooled_height;\n      const int_tp c = (index / pooled_width / pooled_height) % channels;\n      const int_tp n = index / pooled_width / pooled_height / channels;\n      int_tp hstart = ph * stride_h - pad_h;\n      int_tp wstart = pw * stride_w - pad_w;\n      int_tp hend = min(hstart + kernel_h, height + pad_h);\n      int_tp wend = min(wstart + kernel_w, width + pad_w);\n      const int_tp pool_size = (hend - hstart) * (wend - wstart);\n      hstart = max(hstart, 0L);\n      wstart = max(wstart, 0L);\n      hend = min(hend, height);\n      wend = min(wend, width);\n      Dtype aveval = 0;\n      __global const Dtype* bottom_slice = bottom_data\n          + (n * channels + c) * height * width;\n      for (int_tp h = hstart; h < hend; ++h) {\n        for (int_tp w = wstart; w < wend; ++w) {\n          aveval += bottom_slice[h * width + w];\n        }\n      }\n      top_data[index] = aveval / pool_size;\n    }\n  }\n}\n\n__kernel void TEMPLATE(sto_pool_forward_train,Dtype)(\n    const int_tp nthreads, __global const Dtype* bottom_data, const int_tp num,\n    const int_tp channels, const int_tp height, const int_tp width,\n    const int_tp pooled_height, const int_tp pooled_width, const int_tp kernel_h,\n    const int_tp kernel_w, const int_tp stride_h, const int_tp stride_w,\n    __global Dtype* rand_idx,\n    __global Dtype* top_data) {\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    const int_tp pw = index % pooled_width;\n    const int_tp ph = (index / pooled_width) % pooled_height;\n    const int_tp c = (index / pooled_width / pooled_height) % channels;\n    const int_tp n = index / pooled_width / pooled_height / channels;\n    const int_tp hstart = ph * stride_h;\n    const int_tp hend = min(hstart + kernel_h, height);\n    const int_tp wstart = pw * stride_w;\n    const int_tp wend = min(wstart + kernel_w, width);\n    Dtype cumsum = 0.;\n    __global const Dtype* bottom_slice = bottom_data\n        + (n * channels + c) * height * width;\n    // First pass: get sum\n    for (int_tp h = hstart; h < hend; ++h) {\n      for (int_tp w = wstart; w < wend; ++w) {\n        cumsum += bottom_slice[h * width + w];\n      }\n    }\n    const float thres = rand_idx[index] * cumsum;\n    // Second pass: get value, and set index.\n    cumsum = 0;\n    for (int_tp h = hstart; h < hend; ++h) {\n      for (int_tp w = wstart; w < wend; ++w) {\n        cumsum += bottom_slice[h * width + w];\n        if (cumsum >= thres) {\n          rand_idx[index] = ((n * channels + c) * height + h) * width + w;\n          top_data[index] = bottom_slice[h * width + w];\n          h = hend;\n          w = wend;\n        }\n      }\n    }\n  }\n}\n\n__kernel void TEMPLATE(sto_pool_forward_test,Dtype)(\n    const int_tp nthreads, __global const Dtype* const bottom_data, const int_tp num,\n    const int_tp channels, const int_tp height, const int_tp width,\n    const int_tp pooled_height, const int_tp pooled_width, const int_tp kernel_h,\n    const int_tp kernel_w, const int_tp stride_h, const int_tp stride_w,\n    __global Dtype* top_data) {\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    const int_tp pw = index % pooled_width;\n    const int_tp ph = (index / pooled_width) % pooled_height;\n    const int_tp c = (index / pooled_width / pooled_height) % channels;\n    const int_tp n = index / pooled_width / pooled_height / channels;\n    const int_tp hstart = ph * stride_h;\n    const int_tp hend = min(hstart + kernel_h, height);\n    const int_tp wstart = pw * stride_w;\n    const int_tp wend = min(wstart + kernel_w, width);\n    // We set cumsum to be 0 to avoid divide-by-zero problems\n    Dtype cumsum = FLT_MIN;\n    Dtype cumvalues = 0.;\n    __global const Dtype* bottom_slice = bottom_data\n        + (n * channels + c) * height * width;\n    // First pass: get sum\n    for (int_tp h = hstart; h < hend; ++h) {\n      for (int_tp w = wstart; w < wend; ++w) {\n        cumsum += bottom_slice[h * width + w];\n        cumvalues += bottom_slice[h * width + w] * bottom_slice[h * width + w];\n      }\n    }\n    top_data[index] = cumvalues / cumsum;\n  }\n}\n\n__kernel void TEMPLATE(max_pool_backward,Dtype)(const int_tp nthreads,\n                                                __global const Dtype* top_diff,\n                                                const int use_mask,\n                                                __global const int_tp* mask,\n                                                __global const Dtype* top_mask,\n                                                const int_tp num,\n                                                const int_tp channels,\n                                                const int_tp height,\n                                                const int_tp width,\n                                                const int_tp pooled_height,\n                                                const int_tp pooled_width,\n                                                const int_tp kernel_h,\n                                                const int_tp kernel_w,\n                                                const int_tp stride_h,\n                                                const int_tp stride_w,\n                                                const int_tp pad_h,\n                                                const int_tp pad_w,\n                                                __global Dtype* bottom_diff) {\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    // find out the local index\n    // find out the local offset\n    const int_tp w = index % width;\n    const int_tp h = (index / width) % height;\n    const int_tp c = (index / width / height) % channels;\n    const int_tp n = index / width / height / channels;\n    const int_tp phstart =\n        (h + pad_h < kernel_h) ? 0 : (h + pad_h - kernel_h) / stride_h + 1;\n    const int_tp phend = min((h + pad_h) / stride_h + 1, pooled_height);\n    const int_tp pwstart =\n        (w + pad_w < kernel_w) ? 0 : (w + pad_w - kernel_w) / stride_w + 1;\n    const int_tp pwend = min((w + pad_w) / stride_w + 1, pooled_width);\n    Dtype gradient = 0;\n    const int_tp offset = (n * channels + c) * pooled_height * pooled_width;\n    __global const Dtype* top_diff_slice = top_diff + offset;\n    if (use_mask == 1) {\n      __global const int_tp* mask_slice = mask + offset;\n      for (int_tp ph = phstart; ph < phend; ++ph) {\n        for (int_tp pw = pwstart; pw < pwend; ++pw) {\n          if (mask_slice[ph * pooled_width + pw] == h * width + w) {\n            gradient += top_diff_slice[ph * pooled_width + pw];\n          }\n        }\n      }\n    } else {\n      __global const Dtype* top_mask_slice = top_mask + offset;\n      for (int_tp ph = phstart; ph < phend; ++ph) {\n        for (int_tp pw = pwstart; pw < pwend; ++pw) {\n          if (top_mask_slice[ph * pooled_width + pw] == h * width + w) {\n            gradient += top_diff_slice[ph * pooled_width + pw];\n          }\n        }\n      }\n    }\n    bottom_diff[index] = gradient;\n  }\n}\n\n__kernel void TEMPLATE(ave_pool_backward,Dtype)(const int_tp nthreads,\n                                                __global const Dtype* top_diff,\n                                                const int_tp num,\n                                                const int_tp channels,\n                                                const int_tp height,\n                                                const int_tp width,\n                                                const int_tp pooled_height,\n                                                const int_tp pooled_width,\n                                                const int_tp kernel_h,\n                                                const int_tp kernel_w,\n                                                const int_tp stride_h,\n                                                const int_tp stride_w,\n                                                const int_tp pad_h,\n                                                const int_tp pad_w,\n                                                __global Dtype* bottom_diff) {\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    // find out the local index\n    // find out the local offset\n    const int_tp w = index % width + pad_w;\n    const int_tp h = (index / width) % height + pad_h;\n    const int_tp c = (index / width / height) % channels;\n    const int_tp n = index / width / height / channels;\n    const int_tp phstart = (h < kernel_h) ? 0 : (h - kernel_h) / stride_h + 1;\n    const int_tp phend = min(h / stride_h + 1, pooled_height);\n    const int_tp pwstart = (w < kernel_w) ? 0 : (w - kernel_w) / stride_w + 1;\n    const int_tp pwend = min(w / stride_w + 1, pooled_width);\n    Dtype gradient = 0;\n    __global const Dtype* const top_diff_slice = top_diff\n        + (n * channels + c) * pooled_height * pooled_width;\n    for (int_tp ph = phstart; ph < phend; ++ph) {\n      for (int_tp pw = pwstart; pw < pwend; ++pw) {\n        // figure out the pooling size\n        int_tp hstart = ph * stride_h - pad_h;\n        int_tp wstart = pw * stride_w - pad_w;\n        int_tp hend = min(hstart + kernel_h, height + pad_h);\n        int_tp wend = min(wstart + kernel_w, width + pad_w);\n        int_tp pool_size = (hend - hstart) * (wend - wstart);\n        gradient += top_diff_slice[ph * pooled_width + pw] / pool_size;\n      }\n    }\n    bottom_diff[index] = gradient;\n  }\n}\n\n__kernel void TEMPLATE(sto_pool_backward,Dtype)(\n    const int_tp nthreads, __global const Dtype* rand_idx,\n    __global const Dtype* const top_diff, const int_tp num, const int_tp channels,\n    const int_tp height, const int_tp width, const int_tp pooled_height,\n    const int_tp pooled_width, const int_tp kernel_h, const int_tp kernel_w,\n    const int_tp stride_h, const int_tp stride_w, __global Dtype* bottom_diff) {\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    // find out the local index\n    // find out the local offset\n    const int_tp w = index % width;\n    const int_tp h = (index / width) % height;\n    const int_tp c = (index / width / height) % channels;\n    const int_tp n = index / width / height / channels;\n    const int_tp phstart = (h < kernel_h) ? 0 : (h - kernel_h) / stride_h + 1;\n    const int_tp phend = min(h / stride_h + 1, pooled_height);\n    const int_tp pwstart = (w < kernel_w) ? 0 : (w - kernel_w) / stride_w + 1;\n    const int_tp pwend = min(w / stride_w + 1, pooled_width);\n    Dtype gradient = 0;\n    __global const Dtype* rand_idx_slice = rand_idx\n        + (n * channels + c) * pooled_height * pooled_width;\n    __global const Dtype* top_diff_slice = top_diff\n        + (n * channels + c) * pooled_height * pooled_width;\n    for (int_tp ph = phstart; ph < phend; ++ph) {\n      for (int_tp pw = pwstart; pw < pwend; ++pw) {\n        gradient += top_diff_slice[ph * pooled_width + pw]\n            * (index == (int_tp) (rand_idx_slice[ph * pooled_width + pw]));\n      }\n    }\n    bottom_diff[index] = gradient;\n  }\n}";  // NOLINT
std::string pooling_nd_double = "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(max_pool_forward_nd, Dtype)(const int_tp n,\n                                                   const int_tp num_axes,\n                                                   __global const Dtype* bottom_data,\n                                                   const int_tp channels,\n                                                   __global const int_tp* size,\n                                                   __global const int_tp* pooled_size,\n                                                   __global const int_tp* kernel_size,\n                                                   __global const int_tp* ext_kernel_size,\n                                                   __global const int_tp* stride,\n                                                   __global const int_tp* kstride,\n                                                   __global const int_tp* pad,\n                                                   __global Dtype* top_data,\n                                                   const int use_mask,\n                                                   __global int_tp* mask, __global Dtype* top_mask) {\n  int_tp d_idx[6];\n  int_tp d_start[6];\n  int_tp d_end[6];\n  int_tp d_iter[6];\n  int_tp i;\n\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    int_tp offset = 1;\n    int_tp num = index;\n\n    bool do_continue = false;\n\n    for (i = num_axes - 1; i >= 0; --i) {\n      d_idx[i] = num % pooled_size[i];\n      d_start[i] = d_idx[i] * stride[i] - pad[i];\n      d_end[i] = min(d_start[i] + ext_kernel_size[i], size[i]);\n      d_start[i] = max(d_start[i], 0L);\n      num /= pooled_size[i];\n      offset *= size[i];\n      d_iter[i] = d_start[i];\n\n      if (d_start[i] >= d_end[i]) {\n        top_data[index] = -FLT_MAX;\n        if (use_mask) {\n          mask[index] = -1;\n        } else {\n          top_mask[index] = -1;\n        }\n        do_continue = true;\n      }\n    }\n\n    if(do_continue) {\n      continue;\n    }\n\n    int_tp chan = num % channels;\n    num /= channels;\n    offset *= (num * channels + chan);\n\n    Dtype maxval = -FLT_MAX;\n    int_tp maxidx = -1;\n    int_tp final_offset = 0;\n\n    bool incremented;\n    do {\n      final_offset = offset;\n      int_tp size_prod = 1;\n      for (i = num_axes - 1; i >= 0; --i) {\n        final_offset += d_iter[i] * size_prod;\n        size_prod *= size[i];\n      }\n\n      if (bottom_data[final_offset] > maxval) {\n        maxidx = final_offset;\n        maxval = bottom_data[maxidx];\n      }\n\n      incremented = false;\n      for (i = num_axes - 1; i >= 0; --i) {\n        if (d_iter[i] >= d_end[i] - kstride[i]) {\n          d_iter[i] = d_start[i];\n        } else {\n          d_iter[i] += kstride[i];\n          incremented = true;\n          break;\n        }\n      }\n    } while (incremented);\n\n    top_data[index] = maxval;\n    if (use_mask == 1) {\n      mask[index] = maxidx;\n    } else {\n      top_mask[index] = maxidx;\n    }\n  }\n}\n\n\n__kernel void TEMPLATE(max_pool_backward_nd, Dtype)(const int_tp n,\n                                                    const int_tp num_axes,\n                                                    __global const Dtype* top_diff,\n                                                    const int use_mask,\n                                                    __global const int_tp* mask,\n                                                    __global const Dtype* top_mask,\n                                                    const int_tp channels,\n                                                    __global const int_tp* size,\n                                                    __global const int_tp* pooled_size,\n                                                    __global const int_tp* kernel_size,\n                                                    __global const int_tp* ext_kernel_size,\n                                                    __global const int_tp* stride,\n                                                    __global const int_tp* kstride,\n                                                    __global const int_tp* pad,\n                                                    __global Dtype* bottom_diff) {\n  int_tp d_idx[6];\n  int_tp d_start[6];\n  int_tp d_end[6];\n  int_tp d_iter[6];\n  int_tp i;\n\n  for (int_tp index = get_global_id(0); index < n; index += get_global_size(0)) {\n    // find out the local index\n    // find out the local offset\n    int_tp offset = 1;\n    int_tp num = index;\n    for (i = num_axes - 1; i >= 0; --i) {\n      d_idx[i] = num % size[i];\n      if (kstride[i] > 1) {\n        d_start[i] =\n            (d_idx[i] < ext_kernel_size[i]) ?\n                d_idx[i] % kstride[i] : (d_idx[i] - ext_kernel_size[i]) + 1;\n        d_end[i] =\n            (d_idx[i] >= pooled_size[i]) ?\n                (pooled_size[i] - 1)\n                    - (pooled_size[i] - 1 - d_start[i]) % kstride[i] :\n                d_idx[i];\n      } else {\n        d_start[i] =\n            (d_idx[i] + pad[i] < kernel_size[i]) ?\n                0 : (d_idx[i] + pad[i] - kernel_size[i]) / stride[i] + 1;\n        d_end[i] = min((int_tpc) ((d_idx[i] + pad[i]) / stride[i] + 1),\n                       (int_tpc) (pooled_size[i]));\n      }\n      num /= size[i];\n      offset *= pooled_size[i];\n      d_iter[i] = d_start[i];\n\n      if (d_start[i] > d_end[i]) {\n        bottom_diff[index] = 0;\n        return;\n      }\n    }\n    int_tp chan = num % channels;\n    num /= channels;\n    offset *= (num * channels + chan);\n\n    Dtype gradient = 0;\n    int_tp final_offset = 0;\n    int_tp im_offset = 0;\n\n    bool incremented;\n    do {\n      final_offset = offset;\n      im_offset = 0;\n      int_tp size_prod = 1;\n      int_tp pooled_size_prod = 1;\n      for (i = num_axes - 1; i >= 0; --i) {\n        final_offset += d_iter[i] * pooled_size_prod;\n        im_offset += d_idx[i] * size_prod;\n        size_prod *= size[i];\n        pooled_size_prod *= pooled_size[i];\n      }\n\n      if (use_mask) {\n        if (mask[final_offset] == im_offset) {\n          gradient += top_diff[final_offset];\n        }\n      } else {\n        if (top_mask[final_offset] == im_offset) {\n          gradient += top_diff[final_offset];\n        }\n      }\n\n      incremented = false;\n      for (i = num_axes - 1; i >= 0; --i) {\n        if (d_iter[i] > d_end[i] - kstride[i]) {\n          d_iter[i] = d_start[i];\n        } else {\n          d_iter[i] += kstride[i];\n          incremented = true;\n          break;\n        }\n      }\n    } while (incremented);\n    bottom_diff[index] = gradient;\n  }\n}";  // NOLINT
std::string pooling_sk_double = "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(max_pool_forward_sk,Dtype)(const int_tp nthreads,\n__global Dtype* bottom_data,\n                                                  const int_tp num,\n                                                  const int_tp channels,\n                                                  const int_tp height,\n                                                  const int_tp width,\n                                                  const int_tp pooled_height,\n                                                  const int_tp pooled_width,\n                                                  const int_tp kernel_h,\n                                                  const int_tp kernel_w,\n                                                  const int_tp ext_kernel_h,\n                                                  const int_tp ext_kernel_w,\n                                                  const int_tp stride_h,\n                                                  const int_tp stride_w,\n                                                  const int_tp kstride_h,\n                                                  const int_tp kstride_w,\n                                                  const int_tp pad_h,\n                                                  const int_tp pad_w,\n                                                  __global Dtype* top_data,\n                                                  const int use_mask,\n                                                  __global int_tp* mask,\n                                                  __global Dtype* top_mask) {\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    int_tp pw = index % pooled_width;\n    int_tp ph = (index / pooled_width) % pooled_height;\n    int_tp c = (index / pooled_width / pooled_height) % channels;\n    int_tp n = index / pooled_width / pooled_height / channels;\n    int_tp hstart = ph * stride_h - pad_h;\n    int_tp wstart = pw * stride_w - pad_w;\n    int_tp hend = min(hstart + ext_kernel_h, height);\n    int_tp wend = min(wstart + ext_kernel_w, width);\n    hstart = max(hstart, (int_tp) 0);\n    wstart = max(wstart, (int_tp) 0);\n    Dtype maxval = -FLT_MAX;\n    int_tp maxidx = -1;\n    __global Dtype* bottom_data_ptr = bottom_data\n        + (n * channels + c) * height * width;\n    for (int_tp h = hstart; h < hend; h += kstride_h) {\n      for (int_tp w = wstart; w < wend; w += kstride_w) {\n        if (bottom_data_ptr[h * width + w] > maxval) {\n          maxidx = h * width + w;\n          maxval = bottom_data_ptr[maxidx];\n        }\n      }\n    }\n    top_data[index] = maxval;\n    if (use_mask == 1) {\n      mask[index] = maxidx;\n    } else {\n      top_mask[index] = maxidx;\n    }\n  }\n}\n\n__kernel void TEMPLATE(max_pool_backward_sk,Dtype)(\n    const int_tp nthreads, __global const Dtype* top_diff, const int use_mask,\n    __global const int_tp* mask, __global const Dtype* top_mask, const int_tp num,\n    const int_tp channels, const int_tp height, const int_tp width,\n    const int_tp pooled_height, const int_tp pooled_width, const int_tp kernel_h,\n    const int_tp kernel_w, const int_tp ext_kernel_h, const int_tp ext_kernel_w,\n    const int_tp stride_h, const int_tp stride_w, const int_tp kstride_h,\n    const int_tp kstride_w, const int_tp pad_h, const int_tp pad_w,\n    __global Dtype* bottom_diff) {\n\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n\n    __global const int_tp* mask_ptr = mask;\n    __global const Dtype* top_diff_ptr = top_diff;\n\n// find out the local index\n// find out the local offset\n    int_tp w = index % width;\n    int_tp h = (index / width) % height;\n    int_tp c = (index / width / height) % channels;\n    int_tp n = index / width / height / channels;\n\n    int_tp pooled_height_1 = pooled_height - 1;\n    int_tp pooled_width_1 = pooled_width - 1;\n    int_tp phstart = (h < ext_kernel_h) ? h % kstride_h : (h - ext_kernel_h) + 1;\n    int_tp phend =\n        (h >= pooled_height) ?\n            pooled_height_1 - (pooled_height_1 - phstart) % kstride_h : h;\n    int_tp pwstart = (w < ext_kernel_w) ? w % kstride_w : (w - ext_kernel_w) + 1;\n    int_tp pwend =\n        (w >= pooled_width) ?\n            pooled_width_1 - (pooled_width_1 - pwstart) % kstride_w : w;\n\n    Dtype gradient = 0;\n    int_tp offset = (n * channels + c) * pooled_height * pooled_width;\n    top_diff_ptr += offset;\n    if (use_mask == 1) {\n      mask_ptr += offset;\n      for (int_tp ph = phstart; ph <= phend; ph += kstride_h) {\n        for (int_tp pw = pwstart; pw <= pwend; pw += kstride_w) {\n          if (mask_ptr[ph * pooled_width + pw] == h * width + w) {\n            gradient += top_diff_ptr[ph * pooled_width + pw];\n          }\n        }\n      }\n    } else {\n      for (int_tp ph = phstart; ph <= phend; ph += kstride_h) {\n        for (int_tp pw = pwstart; pw <= pwend; pw += kstride_w) {\n          if (top_mask[ph * pooled_width + pw] == h * width + w) {\n            gradient += top_diff_ptr[ph * pooled_width + pw];\n          }\n        }\n      }\n    }\n    bottom_diff[index] = gradient;\n  }\n}\n\n__kernel void TEMPLATE(ave_pool_forward_sk,Dtype)(\n    const int_tp nthreads, __global const Dtype* bottom_data, const int_tp num,\n    const int_tp channels, const int_tp height, const int_tp width,\n    const int_tp pooled_height, const int_tp pooled_width, const int_tp kernel_h,\n    const int_tp kernel_w, const int_tp ext_kernel_h, const int_tp ext_kernel_w,\n    const int_tp stride_h, const int_tp stride_w, const int_tp kstride_h,\n    const int_tp kstride_w, const int_tp pad_h, const int_tp pad_w,\n    __global Dtype* top_data) {\n\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n\n    int_tp pw = index % pooled_width;\n    int_tp ph = (index / pooled_width) % pooled_height;\n    int_tp c = (index / pooled_width / pooled_height) % channels;\n    int_tp n = index / pooled_width / pooled_height / channels;\n    int_tp hstart = ph * stride_h - pad_h;\n    int_tp wstart = pw * stride_w - pad_w;\n    int_tp hend = min(hstart + ext_kernel_h, height + pad_h);\n    int_tp wend = min(wstart + ext_kernel_w, width + pad_w);\n    hstart = max(hstart, 0L);\n    wstart = max(wstart, 0L);\n    hend = min(hend, height);\n    wend = min(wend, width);\n    Dtype aveval = 0;\n    __global const Dtype* bottom_data_ptr = bottom_data;\n    bottom_data_ptr += (n * channels + c) * height * width;\n    int_tp pool_size = 0;\n    for (int_tp h = hstart; h < hend; ++h) {\n      for (int_tp w = wstart; w < wend; ++w) {\n        aveval += bottom_data_ptr[h * width + w];\n        ++pool_size;\n      }\n    }\n    top_data[index] = aveval / pool_size;\n  }\n}\n\n__kernel void TEMPLATE(sto_pool_forward_train_sk,Dtype)(\n    const int_tp nthreads, __global const Dtype* bottom_data, const int_tp num,\n    const int_tp channels, const int_tp height, const int_tp width,\n    const int_tp pooled_height, const int_tp pooled_width, const int_tp kernel_h,\n    const int_tp kernel_w, const int_tp ext_kernel_h, const int_tp ext_kernel_w,\n    const int_tp stride_h, const int_tp stride_w, const int_tp kstride_h,\n    const int_tp kstride_w, __global Dtype* rand_idx,\n    __global Dtype* top_data) {\n\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    int_tp pw = index % pooled_width;\n    int_tp ph = (index / pooled_width) % pooled_height;\n    int_tp c = (index / pooled_width / pooled_height) % channels;\n    int_tp n = index / pooled_width / pooled_height / channels;\n    int_tp hstart = ph * stride_h;\n    int_tp hend = min(hstart + ext_kernel_h, height);\n    int_tp wstart = pw * stride_w;\n    int_tp wend = min(wstart + ext_kernel_w, width);\n    Dtype cumsum = 0.;\n    __global const Dtype* bottom_data_ptr = bottom_data;\n    bottom_data_ptr += (n * channels + c) * height * width;\n    // First pass: get sum\n    for (int_tp h = hstart; h < hend; h += kstride_h) {\n      for (int_tp w = wstart; w < wend; w += kstride_w) {\n        cumsum += bottom_data_ptr[h * width + w];\n      }\n    }\n    float thres = rand_idx[index] * cumsum;\n    // Second pass: get value, and set index.\n    cumsum = 0;\n    for (int_tp h = hstart; h < hend; h += kstride_h) {\n      for (int_tp w = wstart; w < wend; w += kstride_w) {\n        cumsum += bottom_data_ptr[h * width + w];\n        if (cumsum >= thres) {\n          rand_idx[index] = ((n * channels + c) * height + h) * width + w;\n          top_data[index] = bottom_data_ptr[h * width + w];\n          h = hend;\n          w = wend;\n        }\n      }\n    }\n  }\n}\n\n__kernel void TEMPLATE(sto_pool_forward_test_sk,Dtype)(\n    const int_tp nthreads, __global const Dtype* bottom_data, const int_tp num,\n    const int_tp channels, const int_tp height, const int_tp width,\n    const int_tp pooled_height, const int_tp pooled_width, const int_tp kernel_h,\n    const int_tp kernel_w, const int_tp ext_kernel_h, const int_tp ext_kernel_w,\n    const int_tp stride_h, const int_tp stride_w, const int_tp kstride_h,\n    const int_tp kstride_w,\n    __global Dtype* top_data) {\n\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    int_tp pw = index % pooled_width;\n    int_tp ph = (index / pooled_width) % pooled_height;\n    int_tp c = (index / pooled_width / pooled_height) % channels;\n    int_tp n = index / pooled_width / pooled_height / channels;\n    int_tp hstart = ph * stride_h;\n    int_tp hend = min(hstart + ext_kernel_h, height);\n    int_tp wstart = pw * stride_w;\n    int_tp wend = min(wstart + ext_kernel_w, width);\n    // We set cumsum to be 0 to avoid divide-by-zero problems\n    Dtype cumsum = FLT_MIN;\n    Dtype cumvalues = 0.;\n    __global const Dtype* bottom_data_ptr = bottom_data;\n    bottom_data_ptr += (n * channels + c) * height * width;\n    // First pass: get sum\n    for (int_tp h = hstart; h < hend; h += kstride_h) {\n      for (int_tp w = wstart; w < wend; w += kstride_w) {\n        cumsum += bottom_data_ptr[h * width + w];\n        cumvalues += bottom_data_ptr[h * width + w]\n            * bottom_data_ptr[h * width + w];\n      }\n    }\n    top_data[index] = cumvalues / cumsum;\n  }\n\n}";  // NOLINT
std::string slice_double = "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(slice,Dtype)(const int_tp nthreads,\n                                    __global const Dtype* in_data,\n                                    const int forward, const int_tp num_slices,\n                                    const int_tp slice_size,\n                                    const int_tp bottom_slice_axis,\n                                    const int_tp top_slice_axis,\n                                    const int_tp offset_slice_axis,\n                                    __global Dtype* out_data) {\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    const int_tp total_slice_size = slice_size * top_slice_axis;\n    const int_tp slice_num = index / total_slice_size;\n    const int_tp slice_index = index % total_slice_size;\n    const int_tp bottom_index = slice_index\n        + (slice_num * bottom_slice_axis + offset_slice_axis) * slice_size;\n    if (forward == 1) {\n      out_data[index] = in_data[bottom_index];\n    } else {\n      out_data[bottom_index] = in_data[index];\n    }\n  }\n}";  // NOLINT
std::string softmax_loss_double = "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n__kernel void TEMPLATE(softmax_loss_forward,Dtype)(\n    int_tp n, __global const Dtype* prob_data, __global const Dtype* label,\n    __global Dtype* loss,\n    const int_tp num, const int_tp dim, const int_tp spatial_dim,\n    const int has_ignore_label_, const int_tp ignore_label_,\n    __global Dtype* counts) {\n\n  for (int_tp index = get_global_id(0); index < n;\n      index += get_global_size(0)) {\n    const int_tp n = index / spatial_dim;\n    const int_tp s = index % spatial_dim;\n    const int_tp label_value = (int_tp) (label[n * spatial_dim + s]);\n    if (has_ignore_label_ == 1 && label_value == ignore_label_) {\n      loss[index] = 0;\n      counts[index] = 0;\n    } else {\n      loss[index] = -log(\n          max((Dtype) (prob_data[n * dim + label_value * spatial_dim + s]),\n              (Dtype) FLT_MIN));\n      counts[index] = 1;\n    }\n  }\n}\n\n__kernel void TEMPLATE(softmax_loss_backward,Dtype)(const int_tp nthreads,\n                                                    __global const Dtype* top,\n                                                    __global const Dtype* label,\n                                                    __global Dtype* bottom_diff,\n                                                    const int_tp num,\n                                                    const int_tp dim,\n                                                    const int_tp spatial_dim,\n                                                    const int has_ignore_label_,\n                                                    const int_tp ignore_label_,\n                                                    __global Dtype* counts) {\n\n  const int_tp channels = dim / spatial_dim;\n\n  for (int_tp index = get_global_id(0); index < nthreads; index +=\n      get_global_size(0)) {\n\n    const int_tp n = index / spatial_dim;\n    const int_tp s = index % spatial_dim;\n    const int_tp label_value = (int_tp) (label[n * spatial_dim + s]);\n\n    if (has_ignore_label_ == 1 && label_value == ignore_label_) {\n      for (int_tp c = 0; c < channels; ++c) {\n        bottom_diff[n * dim + c * spatial_dim + s] = 0;\n      }\n      counts[index] = 0;\n    } else {\n      bottom_diff[n * dim + label_value * spatial_dim + s] -= 1;\n      counts[index] = 1;\n    }\n  }\n}";  // NOLINT
std::string tile_double = "#ifndef __OPENCL_VERSION__\n#include \"header.cl\"\n#endif\n\n\n__kernel void TEMPLATE(tile,Dtype)(const int_tp nthreads, __global const Dtype* bottom_data,\n                                   const int_tp tile_size, const int_tp num_tiles,\n                                   const int_tp bottom_tile_axis,\n                                   __global Dtype* top_data) {\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    const int_tp d = index % tile_size;\n    const int_tp b = (index / tile_size / num_tiles) % bottom_tile_axis;\n    const int_tp n = index / tile_size / num_tiles / bottom_tile_axis;\n    const int_tp bottom_index = (n * bottom_tile_axis + b) * tile_size + d;\n    top_data[index] = bottom_data[bottom_index];\n  }\n}\n\n\n__kernel void TEMPLATE(tile_backward,Dtype)(const int_tp nthreads,\n                                            __global const Dtype* top_diff,\n                                            const int_tp tile_size,\n                                            const int_tp num_tiles,\n                                            const int_tp bottom_tile_axis,\n                                            __global Dtype* bottom_diff) {\n  for (int_tp index = get_global_id(0); index < nthreads;\n      index += get_global_size(0)) {\n    const int_tp d = index % tile_size;\n    const int_tp b = (index / tile_size) % bottom_tile_axis;\n    const int_tp n = index / tile_size / bottom_tile_axis;\n    bottom_diff[index] = 0;\n    int_tp top_index = (n * num_tiles * bottom_tile_axis + b) * tile_size + d;\n    for (int_tp t = 0; t < num_tiles; ++t) {\n      bottom_diff[index] += top_diff[top_index];\n      top_index += bottom_tile_axis * tile_size;\n    }\n  }\n}";  // NOLINT
viennacl::ocl::program & RegisterKernels(viennacl::ocl::context *ctx) {
  std::stringstream ss;
  ss << header << "\n\n";  // NOLINT
  ss << "#define Dtype float" << "\n\n";  // NOLINT
  ss << "#define TYPE TYPE_FLOAT" << "\n\n";  // NOLINT
  ss << activation_float << "\n\n";  // NOLINT
  ss << auxiliary_float << "\n\n";  // NOLINT
  ss << batch_reindex_float << "\n\n";  // NOLINT
  ss << bnll_float << "\n\n";  // NOLINT
  ss << channel_float << "\n\n";  // NOLINT
  ss << concat_float << "\n\n";  // NOLINT
  ss << contrastive_loss_float << "\n\n";  // NOLINT
  ss << dropout_float << "\n\n";  // NOLINT
  ss << eltwise_float << "\n\n";  // NOLINT
  ss << embed_float << "\n\n";  // NOLINT
  ss << fillbuffer_float << "\n\n";  // NOLINT
  ss << im2col_float << "\n\n";  // NOLINT
  ss << im2col_nd_float << "\n\n";  // NOLINT
  ss << im2col_ndsk_float << "\n\n";  // NOLINT
  ss << im2col_sk_float << "\n\n";  // NOLINT
  ss << lrn_float << "\n\n";  // NOLINT
  ss << math_float << "\n\n";  // NOLINT
  ss << mergecrop_float << "\n\n";  // NOLINT
  ss << pooling_float << "\n\n";  // NOLINT
  ss << pooling_nd_float << "\n\n";  // NOLINT
  ss << pooling_sk_float << "\n\n";  // NOLINT
  ss << slice_float << "\n\n";  // NOLINT
  ss << softmax_loss_float << "\n\n";  // NOLINT
  ss << tile_float << "\n\n";  // NOLINT
  ss << "#ifdef DOUBLE_SUPPORT_AVAILABLE" << "\n\n";  // NOLINT
  ss << "#undef Dtype" << "\n\n";  // NOLINT
  ss << "#define Dtype double" << "\n\n";  // NOLINT
  ss << "#undef TYPE" << "\n\n";  // NOLINT
  ss << "#define TYPE TYPE_DOUBLE" << "\n\n";  // NOLINT
  ss << activation_double << "\n\n";  // NOLINT
  ss << auxiliary_double << "\n\n";  // NOLINT
  ss << batch_reindex_double << "\n\n";  // NOLINT
  ss << bnll_double << "\n\n";  // NOLINT
  ss << channel_double << "\n\n";  // NOLINT
  ss << concat_double << "\n\n";  // NOLINT
  ss << contrastive_loss_double << "\n\n";  // NOLINT
  ss << dropout_double << "\n\n";  // NOLINT
  ss << eltwise_double << "\n\n";  // NOLINT
  ss << embed_double << "\n\n";  // NOLINT
  ss << fillbuffer_double << "\n\n";  // NOLINT
  ss << im2col_double << "\n\n";  // NOLINT
  ss << im2col_nd_double << "\n\n";  // NOLINT
  ss << im2col_ndsk_double << "\n\n";  // NOLINT
  ss << im2col_sk_double << "\n\n";  // NOLINT
  ss << lrn_double << "\n\n";  // NOLINT
  ss << math_double << "\n\n";  // NOLINT
  ss << mergecrop_double << "\n\n";  // NOLINT
  ss << pooling_double << "\n\n";  // NOLINT
  ss << pooling_nd_double << "\n\n";  // NOLINT
  ss << pooling_sk_double << "\n\n";  // NOLINT
  ss << slice_double << "\n\n";  // NOLINT
  ss << softmax_loss_double << "\n\n";  // NOLINT
  ss << tile_double << "\n\n";  // NOLINT
  ss << "#endif" << "\n\n";
  std::string kernel_string = ss.str();
  const char* kernel_program = kernel_string.c_str();
  // ctx->build_options("-cl-fast-relaxed-math -cl-mad-enable");
  viennacl::ocl::program &program = ctx->add_program(kernel_program,
      "kernel_program");
  return program;
}
}  // namespace caffe
#endif
